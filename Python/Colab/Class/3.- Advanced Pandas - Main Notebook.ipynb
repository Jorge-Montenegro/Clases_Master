{"nbformat":4,"nbformat_minor":0,"metadata":{"anaconda-cloud":{},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"},"colab":{"name":"Advanced Pandas - Main Notebook.ipynb","provenance":[{"file_id":"1iM1ECYDqjxYf9mZ66KLfgec_eGbCt4j1","timestamp":1611067595768},{"file_id":"1BBbCIAxlrFxDSyo22fK1i6KycSwwdMvP","timestamp":1610176627313},{"file_id":"1gX1uDo11nxvH5I7ruQBenWuXJtzv1Ybq","timestamp":1608390969992}],"collapsed_sections":[],"toc_visible":true}},"cells":[{"cell_type":"markdown","metadata":{"id":"dGyRS29279MS"},"source":["Let's start by importing the necessary modules. Our usual suspects so far are `numpy`, `os`,  and `pandas` of course. But we'll also be importing `pyplot` from `matplotlib` and we'll proceed to configure it right away so our graphics show up nicely:"]},{"cell_type":"markdown","metadata":{"id":"qFXFuDvkibL0"},"source":["# Introduction to Data Analysis with Python II\n","\n","\n","<img src=\"https://www.python.org/static/img/python-logo.png\" style=\"width: 200px; float: right;\"/>"]},{"cell_type":"markdown","metadata":{"id":"_IWunuRvibL4"},"source":["## Data Wrangling: Clean, Transform, Merge, Reshape"]},{"cell_type":"code","metadata":{"id":"-lglGYsmibL5"},"source":["import numpy as np\n","import os\n","import matplotlib.pyplot as plt\n","plt.rc('figure', figsize=(10, 6))\n","from pandas import Series, DataFrame\n","import pandas as pd\n","\n","%matplotlib inline"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Gs8RLns0ibL5"},"source":["## Combining and merging data sets"]},{"cell_type":"markdown","metadata":{"id":"8C_xA_UJibL6"},"source":["### Database-style DataFrame merges"]},{"cell_type":"markdown","metadata":{"id":"9zrPgUkMnkgW"},"source":["Let's create a couple of quick dataframes from a dictionary as input to illustrate merges:"]},{"cell_type":"code","metadata":{"collapsed":true,"id":"9KvNsor1ibL6"},"source":["df1 = pd.DataFrame({\n","  'data1' : range(7),\n","  'key' : list('bbacaab')\n","})\n","df2 = pd.DataFrame({\n","  'data2' : range(20,23),\n","  'key' : list('abd')\n","})"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lqO27L-j8yKr"},"source":["Everything looks as expected:"]},{"cell_type":"code","metadata":{"id":"OXeU0gmTibL6"},"source":["df1"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-W42It73ibL7"},"source":["df2"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pL_1DKA2ibL7"},"source":["Let's talk about merge.\n","\n","By default, .merge() performs an [inner join](https://www.w3schools.com/sql/sql_join.asp) between the DataFrames, using the common columns as keys. In a database, inner join operation returns recors that have matching values in both tables:\n","\n","![Inner Join](https://drive.google.com/uc?export=view&id=1ONVclC3ZQbsblQG8zwutQZB3JuE6AKF3)\n","\n","Even better, using a diagram that approaches a bit more the representation of a dataframe, the merge operation would be like this:\n","\n","<div>\n","<img src=\"https://i.stack.imgur.com/YvuOa.png\" alt=\"Better Inner Join\" width=\"300\"/>\n","</div>\n","\n","In our case, we're talking about merging elements on the basis of the `key` column. So merging `df2` on `df2` we'll yield a new dataframe based on the `df1` column structure with an additional `data2` column having the row value coming from `df2` corresponding to the values present in `key` in both merged datasets:"]},{"cell_type":"code","metadata":{"id":"AAiI-TeGibL7"},"source":["df1.merge(df2)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Oam4AbhqnySp"},"source":["Reminder: we're not modifying `df1` when invoking `merge()` on it, we're being handed a copy instead:"]},{"cell_type":"code","metadata":{"id":"w7aj0beGnwQO"},"source":["df1"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6m8W5JM5ibL8"},"source":["Inner merge implies that the cartesian product of the elements with common keys is returned. That is, if there are duplicates, it will return all the possible combinations.\n","\n","In set theory, the cartesian product concept is easy. The cartesian product of sets $A$ and $B$ is $A \\times B$, as shown in the image:\n","\n","![Cartesian product of two sets](https://drive.google.com/uc?export=view&id=18ejPhzu4HQMgBl5omgfAYt67pGSTMa38)\n","\n","To see what this yields for our dataframes merging we need to create first a new dataframe with **duplicate** `key` entries:"]},{"cell_type":"code","metadata":{"id":"u7E-EB6gibL8"},"source":["df2_wdups = pd.DataFrame({\n","    'data2' : range(20,24),\n","    'key' : list('abda')\n","    })\n","df2_wdups"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MRs31oUmpv_A"},"source":["When merging `df1` with `df2_wdups` we can see that the cartesian product for the set containing the duplicate elements for a given key is there, as for example is the case for $(a)$ and values $(20,23)$:"]},{"cell_type":"code","metadata":{"id":"mLxNGu3-ibL8"},"source":["df1.merge(df2_wdups)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Hdx9MjvI-Gez"},"source":["So, the cartesian products shown here for the key `a` are the corresponding to a matrix of ([2,4,5]x[20,23])."]},{"cell_type":"markdown","metadata":{"id":"C4tvvm26ibL8"},"source":["If the columns to join on don't have the same name, or we want to join on the index of the DataFrames, we'll need to make it explicit. Let's create a couple of new datasets to show this:"]},{"cell_type":"code","metadata":{"collapsed":true,"id":"Gl6j_e6IibL8"},"source":["df3 = pd.DataFrame({\n","    'data1' : range(7),\n","    'lkey' : list('bbacaab')\n","    })\n","df4 = pd.DataFrame({\n","    'data2' : range(3),\n","    'rkey' : list('abd')\n","    })"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7IdfS8qF-tNi"},"source":["df3"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DwLYgTXa-whm"},"source":["df4"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0IzCQqeqibL9"},"source":["df3.merge(df4,\n","          left_on='lkey',\n","          right_on='rkey')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hL7UJlB5CWvF"},"source":["Why are we naming left and right in this operation? Think about what we saw about merge being an implementation of a SQL JOIN operation.\n","\n","Do you think order matters in these operations? Generally speaking, it does, because the cartesian product is **not** commutative. The same applies for the JOIN operation that merge is doing, with the exception of the inner join where we can see the commutative property taking place:"]},{"cell_type":"code","metadata":{"id":"9EiL_mdgBzAn"},"source":["df4.merge(\n","    df3,left_on='rkey',\n","    right_on='lkey')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DGGlnRUwD9ix"},"source":["You can see, however, that changing the order of the elements of the operation does change the ordering of the result.\n","\n","Let's try now an outer JOIN, that corresponds in set theory to this operation:\n","\n","![full outer join](https://www.w3schools.com/sql/img_fulljoin.gif)\n","\n","Or, again, using a more dataframe-oriented representation:\n","\n","<div>\n","<img src=\"https://i.stack.imgur.com/euLoe.png\" alt=\"Better Full Outer Join\" width=\"300\"/>\n","</div>"]},{"cell_type":"code","metadata":{"id":"PoC7opdSEdeG"},"source":["df1"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7tJirm09Eern"},"source":["df2"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"S1LS2ok7ibL9"},"source":["df1.merge(df2, how='outer')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jnGTEwUWE2e7"},"source":["That was easy according to the set theory diagram (and should be commutative as well), as we didn't have any duplicates in df2. What if we do?:"]},{"cell_type":"code","metadata":{"id":"mCsH2WwRFCz3"},"source":["df2_wdups"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"AnFwyscJFJtC"},"source":["As expected, we'll get the cartesian product:"]},{"cell_type":"code","metadata":{"id":"66uAAQCEEp4-"},"source":["df1.merge(df2_wdups, how='outer')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VVRi27r3F25F"},"source":["We can do a left join as well, which returns all the rows from the left dataframe (`df1`) and the matched rows of the right dataframe (`df2`):\n","\n","![left join](https://www.w3schools.com/sql/img_leftjoin.gif)\n","\n","Or, once again:\n","\n","<div>\n","<img src=\"https://i.stack.imgur.com/BECid.png\" alt=\"Better Full Left Outer Join\" width=\"300\"/>\n","</div>"]},{"cell_type":"code","metadata":{"id":"V-aij0oqibL9"},"source":["df1.merge(df2, how='left')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"p5kXwY_AGvUz"},"source":["You can see that for existing keys in `df1` that are non existent in `df2` (as is the case of `c`) Pandas will fill the corresponding column (`data2`) with `NaN`.\n","\n","If there are two columns with the same name that we do not join on, both will get transferred to the resulting DataFrame with a suffix.\n","\n","Let's modify our dataframes `df1` and `df2` so they both have an extra column with the same name:"]},{"cell_type":"code","metadata":{"id":"_yimstv-ibL9"},"source":["df1['X'] = 2\n","df1"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ibZY5GaKibL9"},"source":["df2['X'] = 42\n","df2"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"eR5umZylibL-"},"source":["...and proceed to do an inner join on the column `key`:"]},{"cell_type":"code","metadata":{"id":"yJ0N76jZibL-"},"source":["df1.merge(df2, on='key')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FZmuaaKQHAdW"},"source":["We see the default naming convention assigns `x` and `y`. We can modify that by being explicit about suffixes:"]},{"cell_type":"code","metadata":{"id":"9EcQv0HfibL-"},"source":["df1.merge(df2, on='key', suffixes=['_left', '_right'])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RctVL3IqibL-"},"source":["### Merging on index\n","\n","So far, we didn't explicitly define an index for our dataframes nor used it for our merging operations. Let's do it, creating a new `df5` for our practice first:"]},{"cell_type":"code","metadata":{"id":"ufHEkshvibL-"},"source":["df5= pd.DataFrame({\n","    'g': range(4),\n","    'h': range(8,12)\n","    },\n","    index =list('abcd'))\n","df5"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BVkY6WC-lnMX"},"source":["Our `df1` was:"]},{"cell_type":"code","metadata":{"id":"sxKtmLzoibL-"},"source":["df1"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Z6r6A82ylrPZ"},"source":["Let's do an inner join where we explicitly mark the index to use in the right set as the dataframe index (option `right_index=True`) "]},{"cell_type":"code","metadata":{"id":"D3epj4JCibL_"},"source":["df1.merge(df5, left_on='key', right_index=True)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1OGK1SixibL_"},"source":["### Concatenating along an axis"]},{"cell_type":"markdown","metadata":{"id":"G_w2RhgrnDKZ"},"source":["We all know merging and concat is **not** the same, but let's make it graphically clear:\n","\n","Merging a dataframe:\n","\n","![merge](https://miro.medium.com/max/1400/1*-uSHoxrzM57syqnKnms2iA.png)\n","\n","Concatenating a dataframe on two different axes:\n","![concat](https://miro.medium.com/max/1400/1*0wu6DunCzPC4o9FIyRTW4w.png)\n","\n","Now that we cleared that out, we can start practicing with dataframes concatenation:"]},{"cell_type":"code","metadata":{"id":"wteFjTeYnxA3"},"source":["df1"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YjcSIFxBnydX"},"source":["df5"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wRO6KZt8ibL_"},"source":["pd.concat([df1, df5])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"psR_mSNrn1Nd"},"source":["Remember that our default axis is axis 0 (operating by rows), and that was that happened here, where Pandas filled in with `NaN` the values it didn't have."]},{"cell_type":"code","metadata":{"id":"O8t421bCibL_"},"source":["import numpy as np\n","\n","a1 = np.arange(0,24).reshape(4,6)\n","a1"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ctzI0Z03ibL_"},"source":["a2 = np.arange(25,37).reshape(4,3)\n","a2"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vQp8naR4ibL_"},"source":["a3 = np.concatenate([a1,a2], axis=1)\n","a3"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"SuptjN49ibMA"},"source":["s1 = pd.Series(range(4), index=list('abcd'))\n","s2 = pd.Series(range(10,13), index=list('lmn'))\n","s3 = pd.Series(range(40,43), index=list('xyz'))\n","print(f{s1},{s2},{s3})"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"nTUfy5pRibMA"},"source":["pd.concat([s1,s2,s3])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zF8MHL8AibMA"},"source":["result = pd.concat([s1,s2,s3], axis=1)\n","result"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"uqjBJydDibMA"},"source":["result = pd.concat([s1,s2,s3], axis=1, keys=['s1', 's2', 's3'])\n","result"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"L8wKEWsSibMA"},"source":["pd.concat([df1,df2], ignore_index=True)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LP_2ESecibMA"},"source":["## On Time Performance Table, transtats.\n","\n","Downloaded from `https://www.transtats.bts.gov/`. Here you have the instructions to download it by yourself, but this notebook takes care of it, so skip this extra step and go to the next cell.\n","\n","### (Optional) Instructions for download\n","\n","Input \"On Time Performance\" in search box, click on \"Airline On-Time Performance Data\" from the search results, then on the bottom right corner of \"Reporting Carrier On-Time Performance (1987-present)\" click \"Download\". In the next screen, click \"Prezipped file\", select the period (March and April 2020), and click \"Download\" once for each period, for a total of 2 zip files."]},{"cell_type":"markdown","metadata":{"id":"CYf49nm2m4H3"},"source":["First, let's mount our Drive in Colab so we can store and persist the files we're going to download:"]},{"cell_type":"code","metadata":{"id":"uPa1vemql7HK"},"source":["import os\n","drive_loc = '/content/gdrive'\n","files_loc = os.path.join(drive_loc, 'MyDrive', 'pdsfiles')\n","\n","from google.colab import drive\n","drive.mount(drive_loc)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3wtnEUdknCAx"},"source":["Now, get the data on airline on-time performance for march 2020:"]},{"cell_type":"code","metadata":{"id":"3IkaVoJsmbLj"},"source":["!wget https://transtats.bts.gov/PREZIP/On_Time_Reporting_Carrier_On_Time_Performance_1987_present_2020_3.zip -P {files_loc}"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bdvwtnd_nJwb"},"source":["...and do the same for april 2020:"]},{"cell_type":"code","metadata":{"id":"v_BCbkhglcH1"},"source":["!wget https://transtats.bts.gov/PREZIP/On_Time_Reporting_Carrier_On_Time_Performance_1987_present_2020_4.zip -P {files_loc}"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SsKO2PLvnSpT"},"source":["Finally, let's unzip both files:"]},{"cell_type":"code","metadata":{"id":"2bA7EvAenY7T"},"source":["!cd {files_loc}; unzip -qq *_3.zip; unzip -qq *_4.zip"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rqg3Pq3LoI0h"},"source":["Let's check that our files are there:"]},{"cell_type":"code","metadata":{"id":"c1jaKkEMnrJm"},"source":["!ls {files_loc}"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fD69q8HmibMB"},"source":["### Take a look at the beginning of the readme file\n","\n","Using the shell:"]},{"cell_type":"code","metadata":{"id":"yEwZtrCdpksU"},"source":["readme_loc = files_loc + '/readme.html'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-KP8tkPKibMB"},"source":["! head {readme_loc}"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fqM58sgtibMB"},"source":["The readme file is HTML. Luckily, we are working in an html environment."]},{"cell_type":"markdown","metadata":{"id":"ZyZv_eLIo-NO"},"source":["### Display the contents of `readme.html` within Colab\n","Use [IPython.display](https://ipython.org/ipython-doc/3/api/generated/IPython.display.html):"]},{"cell_type":"code","metadata":{"id":"2N8wzj0Noa8W"},"source":["from IPython.display import display, HTML\n","display(HTML(filename=readme_loc))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ujI1WAeZibMB"},"source":["That's some very good documentation!\n","\n","The files within the zip are \" quoted csv's. They contain information on timeliness of departures in the US, at the departure level."]},{"cell_type":"markdown","metadata":{"id":"MkRACRDUibMB"},"source":["### Loading the data\n","Let's load one of the files into memory as a pandas dataframe. What functions do you need to use?\n","\n","**Pro tip**: there is no need to decompress the whole file. Check out [zipfile.ZipFile](https://docs.python.org/3/library/zipfile.html)"]},{"cell_type":"markdown","metadata":{"id":"YctCRF92ibMB"},"source":["First, open a connection to one of the files. Let's select the file for March 2020:\n","\n"]},{"cell_type":"code","metadata":{"id":"TrLGM4q0mBrb"},"source":["march_file = !cd {files_loc}; ls {files_loc}/*_3.zip"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"449HwgOemwNZ"},"source":["march_file"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MUSQaQYUmgeR"},"source":["zip_file = march_file[0]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XlDFnXbHibMC"},"source":["import os\n","import zipfile\n","\n","zip_file_handle = zipfile.ZipFile(zip_file)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7_9l6UxQibMC"},"source":["`zip_file` is a connection to the compressed file, the `.zip`. We can use it to open a connection to one of the files it contains, which will behave like a normal uncompressed file that we had opened with `open()`:"]},{"cell_type":"code","metadata":{"id":"0jNLy-mQoYKM"},"source":["type(zip_file_handle)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"RTzXdvKEooOv"},"source":["zip_file_handle.namelist()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"AUj6fS-IibMC"},"source":["csv, readme = zip_file_handle.namelist()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"FYAPtlNZnfXW"},"source":["csv_file = zip_file_handle.open(csv)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"collapsed":true,"id":"sO_vUY7vibMC"},"source":["Now we're ready to load the file into memory as a pandas dataframe. Remember to close the connections to the files!"]},{"cell_type":"code","metadata":{"id":"WvE5vfjmibMC"},"source":["csv_file = zip_file_handle.open(csv)\n","df = pd.read_csv(csv_file)\n","\n","csv_file.close()\n","zip_file_handle.close()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0WoyT0rSpKYw"},"source":["df.head()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"PmJ_Le-FibMC"},"source":["#### Exercise\n","\n","Load both March 2020 and April 2020 into a single DataFrame"]},{"cell_type":"code","metadata":{"id":"MIgzmHYuibMD"},"source":["def get_df_from_zip(zip_filepath):\n","  zip_file_handle = zipfile.ZipFile(zip_filepath)\n","  csv_filename, _ = zip_file_handle.namelist()\n","  csv_file = zip_file_handle.open(csv_filename)\n","  csv_df = pd.read_csv(csv_file)\n","  csv_file.close()\n","  zip_file_handle.close()\n","  return csv_df"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_HvHLr3yrTZv"},"source":["  april_file = !cd {files_loc}; ls {files_loc}/*_4.zip\n","  df_otp = pd.concat([get_df_from_zip(march_file[0]), get_df_from_zip(april_file[0])])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"T_hm9UzvibMD"},"source":["Let's start examining the data: show the beginning of the file. How many records does it contain?"]},{"cell_type":"code","metadata":{"id":"PfivAfHMibMD"},"source":["pd.options.display.max_columns = None"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"CkdE0w5EibMD"},"source":["df_otp.head()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pJFoASayibMD"},"source":["df_otp.shape"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wiev8bWbibMD"},"source":["print(df_otp.size)\n","df_otp.size == df_otp.shape[0] * df_otp.shape[1]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"scrolled":true,"id":"OuM-2IxQibMD"},"source":["df_otp.dtypes"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"u3VArQMaibME"},"source":["#### Digression\n","\n","Attention! Be careful not to reassign to reserved words or functions- you will overwrite the variable."]},{"cell_type":"code","metadata":{"collapsed":true,"id":"2SsRXY4yibME"},"source":["pd.concat = df1"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"g9dhK-QhibME"},"source":["pd.concat([s1,s2])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hbBafUjTibME"},"source":["You can delete the overwritten variable, but you won't get back the original value. If it is an object or function from a module, you'll need to reload() the module, since Python doesn't load again an already imported module if you try to import it. reload() is useful also when you are actively developing your own module and want to load the latest definition of a function into memory."]},{"cell_type":"code","metadata":{"collapsed":true,"id":"bSAPz_RqibME"},"source":["del(pd.concat)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"SJQWf_UcibMF"},"source":["pd.concat"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xnr9zPayibMF"},"source":["import imp\n","imp.reload(pd)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QtFoPFT5ibMF"},"source":["pd.concat"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"AsyIQAvSibMF"},"source":["## Data transformation"]},{"cell_type":"markdown","metadata":{"id":"b4II9PfKibMG"},"source":["### Removing duplicates"]},{"cell_type":"markdown","metadata":{"id":"mWEdBcq-3SCR"},"source":["Let's create a new dataframe to learn about removing duplicates, that obviously contains some:"]},{"cell_type":"code","metadata":{"id":"VDifu9sJibMG"},"source":["df6 = pd.DataFrame({\n","    'key1' : ['one'] * 3 + ['two'] * 4,\n","    'key2' : [1, 1, 2, 3, 3, 4, 4]\n","    })\n","df6"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5pDWpIah3pA2"},"source":["The method `duplicated` will work by default on axis 0 and will return a boolean marking the duplicate entries (rows) in the dataframe:"]},{"cell_type":"code","metadata":{"id":"ByH4fpL8ibMG"},"source":["df6.duplicated()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6ilRV4k33z1Y"},"source":["If you want to act on the duplicates (removing them), then Pandas has the `drop_duplicates` method:"]},{"cell_type":"code","metadata":{"id":"oVykTtVgibMG"},"source":["df6.drop_duplicates()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QmxVog9eibMG"},"source":["df6.drop_duplicates(keep='last', subset = 'key1')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"n3z6FFgeibMG"},"source":["#### Exercise\n","\n","Let's practice with our big dataset by doing some data cleansing/rationalization thanks to duplicates processing. Consider the following questions:\n","\n","- How many individual airports are there in the OTP data?\n","- How many routes (combinations of origin / destination)\n","\n","How would you approach the solution?\n","\n","*Hint: Remove duplicates with `subset`, then `count()`.*"]},{"cell_type":"code","metadata":{"id":"QY1hjI-JkCLd"},"source":["df_otp.drop_duplicates(subset='OriginAirportID').count()['OriginAirportID']"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ev255yAelmGi"},"source":["df_otp.drop_duplicates(subset=['OriginAirportID','DestAirportID']).count()['OriginAirportID']"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"OBaiYUuyibMH"},"source":["### Renaming axis indexes"]},{"cell_type":"markdown","metadata":{"id":"EknwaRH1Vm62"},"source":["Let's add an explicit index to our last dataset (refresh from the previous class):"]},{"cell_type":"code","metadata":{"id":"8sr0z8mIWoQn"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"g6WNAtGIibMH"},"source":["df6.index = list('plfjdmh')\n","df6"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"JM2ArIBuibMH"},"source":["### Discretization and binning"]},{"cell_type":"code","metadata":{"id":"kMLhYeMdWOU4"},"source":["!wget http://bit.ly/ks-pds-csv8 -P {files_loc}"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"F9x6TK8mcMJB"},"source":["Let's rename our file for something more intuitive in the future:"]},{"cell_type":"code","metadata":{"id":"BmUOslYsWSkQ"},"source":["import os\n","orig_filepath = os.path.join(files_loc,\"ks-pds-csv8\")\n","sales_data_filename = \"sales_data.csv\"\n","sales_data_filepath = os.path.join(files_loc, sales_data_filename)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"SjCr_HYSbndE"},"source":["!mv {orig_filepath} {sales_data_filepath}"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_iZt4aF8fiOf"},"source":["Ok, let's load our data now and have a quick look at it:"]},{"cell_type":"code","metadata":{"id":"nWmt-ttNcLgk"},"source":["raw_df = pd.read_csv(sales_data_filepath)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LFhm76YjceFj"},"source":["raw_df.head()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9xDBorqidmCP"},"source":["raw_df.describe()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DZWZfdV4fIVI"},"source":["Let's modify our dataframe a bit doing some group operations. Bear with me with this code for now as we'll be explaining how `groupby` works later on in this notebook:"]},{"cell_type":"code","metadata":{"id":"CNQCO7NBcnBK"},"source":["df = raw_df.groupby(['account number', 'name'])['ext price'].sum().reset_index()\n","\n","df"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"sPdQJ0hbfaqy"},"source":["Now let's ask Seaborn to plot this information for us:"]},{"cell_type":"code","metadata":{"id":"E5x2uTpaddRR"},"source":["import seaborn as sns\n","\n","sns.set_style('whitegrid')\n","\n","df['ext price'].plot(kind='hist')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FFvJ5_gUeklj"},"source":["Here, Seaborn `hist` is showing us 8 bins with data. What if we wanted to divide our customers a different number of groups (or bins)? That’s what pandas `qcut` and `cut` are for.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"2Yxm3RNfGvXV"},"source":["Let's start with `qcut`. `qcut` is a Quantile-based discretization function. This basically means that qcut tries to divide up the underlying data into equal sized bins. The function defines the bins using percentiles based on the distribution of the data, not the actual numeric edges of the bins.\n","\n","Having used pandas describe function, you have already seen an example of the underlying concepts represented by `qcut`\n","\n"]},{"cell_type":"code","metadata":{"id":"fhzRAzC9HGDL"},"source":["df['ext price'].describe()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3q9ZI9sMHotm"},"source":["Keep in mind the values for the 25%, 50% and 75% percentiles, we'll be seeing them again using `qcut`.\n","\n","The simplest use of qcut is to define the number of quantiles and let pandas figure out how to divide up the data. Let's tell pandas to create 4 equal sized groupings of the data:"]},{"cell_type":"code","metadata":{"id":"PQGCu-cyHoXG"},"source":["pd.qcut(df['ext price'], q=4)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-dGuoz7RIAQO"},"source":["The result is a categorical series representing the sales bins. Because we asked for quantiles with `q=4 `the bins match the percentiles from the describe function.\n","\n","A common use case is to store the bin results back in the original dataframe for future analysis. So let's create 4 bins (aka quartiles) and 10 bins (aka deciles) and store the results back in the original dataframe:\n"]},{"cell_type":"code","metadata":{"id":"39MEaqkGIAEt"},"source":["df['quantile_ex_1'] = pd.qcut(df['ext price'], q=4)\n","df['quantile_ex_2'] = pd.qcut(df['ext price'], q=10, precision=0)\n","\n","df.head()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"L0hfjxr-IRQV"},"source":["We can see how the bins are very different between `quantile_ex_1` and `quantile_ex_2` . We also used `precision` to define how many decimal points to use for calculating the bin precision.\n","\n","The other interesting view is to see how the values are distributed across the bins using `value_counts``:"]},{"cell_type":"code","metadata":{"id":"1Vr5U0jFIeAT"},"source":["df['quantile_ex_1'].value_counts()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"M1SNHJfJIRFP"},"source":["df['quantile_ex_2'].value_counts()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"PJtdbQthIm4F"},"source":["This illustrates a key concept. In each case, there are an equal number of observations in each bin. Pandas does the math behind the scenes to figure out how wide to make each bin. If you look closely, in `quantile_ex_1` the range of the first bin is 74,661.15 while the second bin is only 9,861.02 (110132 - 100271).\n","\n","One of the challenges with this approach is that the bin labels are not very easy to explain to an end user. For instance, if we wanted to divide our customers into 5 groups (aka quintiles) like an airline frequent flier approach, we can explicitly label the bins to make them easier to interpret."]},{"cell_type":"code","metadata":{"id":"f9HETs-XIQ63"},"source":["bin_labels_5 = ['Bronze', 'Silver', 'Gold', 'Platinum', 'Diamond']\n","df['quantile_ex_3'] = pd.qcut(df['ext price'],\n","                              q=[0, .2, .4, .6, .8, 1],\n","                              labels=bin_labels_5)\n","df.head()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"639Ukw7rH_61"},"source":["We just did some things a little differently. We explicitly defined the range of quantiles to use: q=[0, .2, .4, .6, .8, 1]\n","\n","But we also defined the labels `labels=bin_labels_5` to use when representing the bins.\n","\n","Let’s check the distribution:"]},{"cell_type":"code","metadata":{"id":"12-08hc_H_tp"},"source":["df['quantile_ex_3'].value_counts()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2ysqObGJJNuX"},"source":["W now have an equal distribution of customers across the 5 bins and the results are displayed in an easy to understand manner.\n","\n","One important item to keep in mind when using `qcut` is that the quantiles must all be less than 1. Here are some examples of distributions. In most cases it’s simpler to just define `q` as an integer:\n","\n","- terciles: q=[0, 1/3, 2/3, 1] or q=3\n","- quintiles: q=[0, .2, .4, .6, .8, 1] or q=5\n","- sextiles: q=[0, 1/6, 1/3, .5, 2/3, 5/6, 1] or q=6\n","\n","Now, how do we know what ranges are used to identify the different bins? We can use `retbins=True` to return the bin labels:"]},{"cell_type":"code","metadata":{"id":"XovyhZnuJf9N"},"source":["results, bin_edges = pd.qcut(df['ext price'],\n","                            q=[0, .2, .4, .6, .8, 1],\n","                            labels=bin_labels_5,\n","                            retbins=True)\n","\n","results_table = pd.DataFrame(zip(bin_edges, bin_labels_5),\n","                            columns=['Threshold', 'Tier'])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"K2LskZijJfyo"},"source":["results_table"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Ng1NrcPlJfmt"},"source":["Now, let's go with `cut`.\n","\n","Many of the concepts we discussed above apply but there are a couple of differences with the usage of `cut`.\n","\n","The major distinction is that `qcut` will calculate the size of each bin in order to make sure the distribution of data in the bins is equal. All bins will roughly have the same number of observations but the bin range will vary.\n","\n","On the other hand, `cut` is used to specifically define the bin edges. There is no guarantee about the distribution of items in each bin. In fact, we can define bins in such a way that no items are included in a bin or nearly all items are in a single bin.\n","\n","In real world examples, bins may be defined by business rules. For a frequent flier program, 25,000 miles is the silver level and that does not vary based on year to year variation of the data. If we want to define the bin edges (25,000 - 50,000, etc) we would use `cut` . We can also use `cut` to define bins that are of constant size and let pandas figure out how to define those bin edges.\n","\n","Let's remove some columns to keep the examples short:"]},{"cell_type":"code","metadata":{"id":"feUzcWSvH_dm"},"source":["df = df.drop(columns = ['quantile_ex_1','quantile_ex_2', 'quantile_ex_3'])\n","df"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"uk77iDLrHoDl"},"source":["For the first example, we can cut the data into 4 equal bin sizes. Pandas will perform the math behind the scenes to determine how to divide the data set into these 4 groups:\n","\n"]},{"cell_type":"code","metadata":{"id":"XvGohDTTKmEg"},"source":["pd.cut(df['ext price'], bins=4)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7yTZLOdGKr3K"},"source":["Let’s look at the distribution:"]},{"cell_type":"code","metadata":{"id":"r5FJfQbAKrrR"},"source":["pd.cut(df['ext price'], bins=4).value_counts()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ueA0hsmdKrgY"},"source":["The first thing to notice is that the bin ranges are all about 32,265 but that the distribution of bin elements is not equal.\n","\n","The bins have a distribution of 12, 5, 2 and 1 item(s) in each bin. This is the essential difference between `cut` and `qcut`."]},{"cell_type":"markdown","metadata":{"id":"ZeWHHJ3BLA20"},"source":["Interval notation:\n","\n","![Intervals](https://pbpython.com/images/Interval_notation.png)"]},{"cell_type":"markdown","metadata":{"id":"SgPVn1_BLLen"},"source":["When using `cut`, we may be defining the exact edges of our bins so it is important to understand if the edges include the values or not. Depending on the data set and specific use case, this may or may not be a big issue. It can certainly be a subtle issue we do need to consider.\n","\n","To bring it into perspective, when we present the results of your analysis to others, we will need to be clear whether an account with 70,000 in sales is a silver or gold customer.\n","\n","Here is an example where we want to specifically define the boundaries of our 4 bins by defining the bins parameter."]},{"cell_type":"code","metadata":{"id":"jTUvtFRCLVbE"},"source":["cut_labels_4 = ['silver', 'gold', 'platinum', 'diamond']\n","cut_bins = [0, 70000, 100000, 130000, 200000]\n","df['cut_ex1'] = pd.cut(df['ext price'], bins=cut_bins, labels=cut_labels_4)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bqa8PGtKKrU-"},"source":["df.head()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HnMWoIZGMGI2"},"source":["Let's make sure we understand how the `cut_bins` are being procesed when making the classification (close or open interval):"]},{"cell_type":"code","metadata":{"id":"-YsosmsgLvY6"},"source":["pd.cut(df['ext price'], bins=cut_bins)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5KthQB_MibMH"},"source":["## String manipulation"]},{"cell_type":"markdown","metadata":{"id":"gJM0Ixu9ibMH"},"source":["### String object methods"]},{"cell_type":"markdown","metadata":{"id":"1dNQfpLagSEm"},"source":["Maybe refreshing some concepts you already show in the Python introductory classes, you know that you can get a list from a string by calling `split()`. This may sound basic, but it will be quite useful for some Data Science text and language processing code you'll see later on in the course:"]},{"cell_type":"code","metadata":{"id":"_o0qDUoribMH"},"source":["string = 'this is some sentence'\n","string.split()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8B0hAf-MibMI"},"source":["### Vectorized string functions in pandas\n","\n","[Vectorized string functions in pandas](https://pandas.pydata.org/pandas-docs/stable/text.html) are grouped within the `.str` attribute of Series and Indexes. They have the same names as the regular Python string functions, but work on Series of strings.\n","\n","We saw in the previous NumPy class (and in the first Pandas class) how both modules generalize arithmetic operations so that we can easily and quickly perform the same operation on many array elements.\n","\n","This is called a *vectorization* of the operations, and simplifies the syntax of operating on arrays of data: we no longer have to worry about the size or shape of the array, but just about what operation we want done."]},{"cell_type":"markdown","metadata":{"id":"Fs8HGFvIkHFO"},"source":["Let's compare working with the array elements (for example, for capitalizing the animal names in this list):"]},{"cell_type":"code","metadata":{"id":"Bnlk8EoYibMI"},"source":["animals = 'rhino giraffe molerat mantisshrimp cheetah mosquito whale'.split()\n","animals"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9vxgPF7DkQ5o"},"source":["Operating over the elements, *in a pythonic way*, involves using a lambda function on `map`:"]},{"cell_type":"code","metadata":{"id":"PO15SXf_ibMI"},"source":["list(map(lambda st: st.capitalize(), animals))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"THLf2nqSkbnL"},"source":["But with vectorized operations, we can be *even more data science pythonic*!:"]},{"cell_type":"code","metadata":{"id":"6en3hQQaibMI"},"source":["df1['animal'] = animals\n","df1"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"FwD1ZzwRibMI"},"source":["animals_series = df1['animal']\n","animals_series.str"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"oO66IzQDibMI"},"source":["animals_series.str.capitalize()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"oTTOjz1FknhA"},"source":["Now, we've just applied the `capitalize()` operation over the components of the series but by syntactically acting on the Series itself.\n","\n","Let's see more examples of it:"]},{"cell_type":"code","metadata":{"id":"ZnjdD2_ZibMJ"},"source":["animals_series.str.len()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zPZD41NIibMJ"},"source":["animals_series.str.count('o')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"UhllNNyZibMJ"},"source":["animals_series.str.contains('m')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2jt7LUbhibMJ"},"source":["df1[animals_series.str.contains('m')]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7FEm1S5zk9AW"},"source":["Having spaces in text information we're trying to clean up is quite common. We can perform blanks clean ups as well using vectorized operations:"]},{"cell_type":"code","metadata":{"id":"NUbRDjECibMJ"},"source":["series_with_blanks = pd.Series(['SDF    ', ' RTTR     ', 'BL   '])\n","series_with_blanks"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LMms4VO3sqXr"},"source":["We can clean up the trailing blanks using `rstrip`:"]},{"cell_type":"code","metadata":{"id":"yi_r9K-_ibMJ"},"source":["series_with_blanks.str.rstrip()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DzCAowvlibMK"},"source":["#### Exercise\n","\n","\n","Let's come back to our On Time Perfomance (OTP) dataset. To practice with the recently explained concepts, do the following:\n","\n","* Generate a list of the columns that have 'Origin' in their name\n","* Show a sample of the values that those columns take."]},{"cell_type":"code","metadata":{"id":"9UqBvuPwibMK"},"source":["df_otp.columns.str.contains('Origin')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3sGwZpbeibMK"},"source":["df_otp.columns[df_otp.columns.str.contains('Origin')]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"scrolled":true,"id":"oOPBE7IuibMK"},"source":["df_otp[df_otp.columns[df_otp.columns.str.contains('Origin')]].sample(5)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rjAZvLnDibMK"},"source":["So much redundant information! Let's jump ahead with this list of interesting columns:\n","\n","```python\n","interesting_columns= ['FlightDate', 'DayOfWeek', 'Reporting_Airline', 'Tail_Number', 'Flight_Number_Reporting_Airline', \n","                      'Origin', 'OriginCityName', 'OriginStateName', 'OriginCityMarketID',\n","                      'Dest', 'DestCityName', 'DestStateName', 'DestCityMarketID',\n","                      'DepTime', 'DepDelay', 'AirTime', 'Distance']\n","\n","flights = flights[interesting_columns]\n","```"]},{"cell_type":"markdown","metadata":{"id":"K4A06ZtJibMK"},"source":["# Data Aggregation and Group Operations"]},{"cell_type":"markdown","metadata":{"id":"H-Ym-Q_5ibMK"},"source":["## GroupBy mechanics\n","\n","Sometimes also called split-apply-combine for talking about group operations, a good description of the process.\n","\n","- **Split**: data contained in a pandas object, whether a Series or DataFrame is split into groups based on one or more keys that you provide. The splitting is performed on a particular axis of an object. For example, a DataFrame can be grouped on its rows (axis=0) or its columns (axis=1).\n","- **Apply**: A function is then applied to each group, producing a new value.\n","- **Combine**: Finally, the results of all those function applications are combined into a result object. The form of the resulting object will usually depend on what’s being done to the data.\n","\n","![Split-Apply-Combine](https://jakevdp.github.io/PythonDataScienceHandbook/figures/03.08-split-apply-combine.png)\n","\n","Let's get started with an example:"]},{"cell_type":"code","metadata":{"id":"y3mY4VFNibML"},"source":["df = DataFrame({'key1' : ['a', 'a', 'b', 'b', 'a'],\n","                'key2' : ['one', 'two', 'one', 'two', 'one'],\n","                'data1' : np.random.randn(5),\n","                'data2' : np.random.randn(5)})\n","df"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"eTfTxnxNxMbX"},"source":["The most basic split-apply-combine operation can be computed with the `groupby()` method of DataFrames and Series, passing the name of the desired key column to group by. In this case, let's practice with a Series:"]},{"cell_type":"code","metadata":{"id":"1kUEUN18ibML"},"source":["grouped = df['data1'].groupby(df['key1'])\n","grouped"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZykQjXfcxAj-"},"source":["We don't get a set of Series, but a SeriesGroupBy object. \n","\n","This object is where the magic is: you can think of it as a special view of the Series, waiting to dig into the groups but doesn't actually compute anything until the aggregation is applied.\n","\n","This \"lazy evaluation\" approach means that common aggregates can be implemented very efficiently in a way that is almost transparent to the user.\n","\n","Also, note that we're telling Pandas to group a Series using the criteria provided by another one (that is **not** the index of the Series we're grouping). This will be standard practice and you'll become more familiar with it as we move forward with this notebook.\n","\n","Let's produce a result to see it:"]},{"cell_type":"code","metadata":{"id":"sMUO6vVuz7n_"},"source":["grouped.sum()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rdpsWbTl2hwv"},"source":["As we expected, we obtain the sum of the rows that correspond to each key (`a` or `b`).\n","\n","If we ask for the mean..."]},{"cell_type":"code","metadata":{"id":"AhDjvsbtibML"},"source":["grouped.mean()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ySowTI6k30T7"},"source":["...what we get instead is the mean of the values of the Series (`data1`) in each of the groups (`a` and `b`).\n","\n","Let's operate now on the dataframe itself:"]},{"cell_type":"code","metadata":{"id":"J49v4nuIibML"},"source":["means = df.groupby(['key1','key2','data2']).mean()\n","means"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7jSNclXAFkN1"},"source":["So we asked for the means considering grouping on `key1`, `key2` and `data2` and that's what we're getting for `data1`. There are no differences between original `data1` values and the mean provided because we have just 1 combination of all three different `groupby` combinations.\n","\n","Let's crate a couple of arrays with data to be used by `groupby()` on the series for the `data1` column:"]},{"cell_type":"code","metadata":{"id":"FYZ1OUygibML"},"source":["states = np.array(['Ohio', 'California', 'California', 'Ohio', 'Ohio'])\n","years = np.array([2005, 2005, 2006, 2005, 2006])\n","df['data1'].groupby([states, years]).mean()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zjIvFC7jGQyO"},"source":["You see here the same mechanism mentioned before, working in this case with Numpy arrays instead of other Series coming from the same DataFrame, but the internal dynamics are the same.\n","\n","If we consolidate the computation on the grouping by asking to apply a funtion, we materialize a new dataframe with the result:"]},{"cell_type":"code","metadata":{"id":"IN6z-tXyibML"},"source":["df.groupby('key1').mean()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fjM8AywJGunu"},"source":["In this case, we got the mean values for both non-`groupby` columns (that is, `data1` and `data2`).\n","\n","**Question**: Where is `key2` column? Why is it not showing up?"]},{"cell_type":"markdown","metadata":{"id":"vyOV98PpMjax"},"source":["To better understand how `groupby()` is generating the groups, let's compare the `head()` function over the dataframe or over the `DataframeGroupBy` object:"]},{"cell_type":"code","metadata":{"id":"ppf8FC27LSQv"},"source":["df.groupby(['key1']).head(1)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ymyD4wrqqFEU"},"source":["What we're seeing here is the first row of each dataframe in the `groupby` group. Compare that to asking for the first row of the original dataframe:"]},{"cell_type":"code","metadata":{"id":"vpgdbuEnLZ5v"},"source":["df.head(1)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HQVBuNIAqXgC"},"source":["If we ask for the first 3 rows of each dataframe in the `groupby, we will see no difference from asking the same from the original dataframe. Why?"]},{"cell_type":"code","metadata":{"id":"l9Dn5T8UqssM"},"source":["df.groupby('key1').head(1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1JEWcpn3qxLY"},"source":["df.head()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BLXkOxlJq4Tv"},"source":["Well, this is because although we're actually getting the first 3 rows of each dataset in the group, the ordering is the same as in the original dataset."]},{"cell_type":"markdown","metadata":{"id":"g752cWQCrKBg"},"source":["We should have a more clear intuition right now about how `groupby()` works. With this in mind, let's do a a new grouping by two keys now and ask to compute the mean over each group:"]},{"cell_type":"code","metadata":{"id":"G1xifbCfibML"},"source":["df.groupby(['key1', 'key2']).mean()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5cW1Qp14sHMA"},"source":["We can ask about the size of each group as well:"]},{"cell_type":"code","metadata":{"id":"eLNMctIeibMM"},"source":["df.groupby(['key1', 'key2']).size()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"iaNq7zsbibMM"},"source":["### Iterating over groups"]},{"cell_type":"markdown","metadata":{"id":"v5jT3GjasOmc"},"source":["We can dig up a bit on what the grouping is doing if we iterate over each of them and print it:"]},{"cell_type":"code","metadata":{"id":"WSPYxu9KibMM"},"source":["for name, group in df.groupby('key1'):\n","    print(name)\n","    print(group)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-lNav7XZsYwT"},"source":["That was kind of easy to visualize without doing the iteration anyway. Let's do it again when we apply a multikey grouping:"]},{"cell_type":"code","metadata":{"id":"6_tWp0YvibMM"},"source":["for (k1, k2), group in df.groupby(['key1', 'key2']):\n","    print((k1, k2))\n","    print(group)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4c7EOe5msngN"},"source":["As we mentioned before, `groupby()` generates a `DataFrameGroupBy` object:"]},{"cell_type":"code","metadata":{"id":"vIUar8wEibMM"},"source":["df.groupby('key1')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1_8hyA1_s8d-"},"source":["Generating a list on it will give us access to the actual elements composing the object, that is, the groups themselves:"]},{"cell_type":"code","metadata":{"id":"ME7f48sDibMM"},"source":["list(df.groupby('key1'))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"M7EdPuE-tGvF"},"source":["Now, to have this even better, let's put it in a Python dictionary and ask about one of its keys:"]},{"cell_type":"code","metadata":{"id":"SxbaR3JzibMN"},"source":["pieces = dict(list(df.groupby('key1')))\n","pieces"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"JWy4Tmg7tSul"},"source":["type(pieces['b'])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"OAX92-tGtdrs"},"source":["So now we have a nicely organized structure where we can see that each member of the group is a dataset where the operation is consequently applied.\n","\n","We can access the dataframe's data types with the property `dtypes`:"]},{"cell_type":"code","metadata":{"id":"2IXLwAibibMN"},"source":["df.dtypes"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"TJ-ic7Vdty4P"},"source":["And then, we can use this property to group our dataframe, selecting the columns instead of the default axis 0. If we apply the same set of data structures transformations that we did before, we'll get the different dataframes in the group, no splitted by types:"]},{"cell_type":"code","metadata":{"id":"fSKSe5nDibMN"},"source":["grouped = df.groupby(df.dtypes, axis=1)\n","dict(list(grouped))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YNGeH8Q8ibMN"},"source":["### Selecting a column or subset of columns"]},{"cell_type":"markdown","metadata":{"id":"zQKv3FfR9MAn"},"source":["To select a column or a subset of columns from the `DataFrameGroupBy`, just pass its name as a string if we want a `SeriesGroupBy` or as an element of a list if we want a `DataFrameGroupBy` instead:"]},{"cell_type":"code","metadata":{"id":"zOGboLKU_16E"},"source":["df.groupby('key1')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PvTGitxhibMN"},"source":["df.groupby('key1')['data1']"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_dLYrFF__erH"},"source":["df.groupby('key1')[['data2']]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"EEySBynD-BaZ"},"source":["You may do the same for Series, for example extracting the Serie from a Dataframe just how we saw earlier in the notebook:\n","\n","> Indented block\n","\n"]},{"cell_type":"code","metadata":{"id":"_OFy5gZDAEuY"},"source":["type(df['data1'])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PbFaz7c0ibMN"},"source":["df['data1'].groupby(df['key1'])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"XvhY95L9ArZD"},"source":["We asked for a `SeriesGroupBy` and this is what we got. It makes sense, because the data structure is 1-dimensional.\n","\n","However, if we pass a non 1-dimensional grouper to the Series, this will not work:"]},{"cell_type":"code","metadata":{"id":"M8mggxPEAYo_"},"source":["df['data1'].groupby(df[['data1']])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DjeUyRhFB4KJ"},"source":["Passing a list to our original datafram will generate a new one though:"]},{"cell_type":"code","metadata":{"id":"V9k3cbnpAORO"},"source":["type(df[['data2']])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nnQhOwrJCFIk"},"source":["...so this will accept a 2-dimensional grouper:"]},{"cell_type":"code","metadata":{"id":"EOJSoIgX_7bg"},"source":["df[['data2']].groupby(df['key1'])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mChcRXoZ-OEv"},"source":["So wrapping up, when we materialize an operation (like the mean in this case), we obtain only the columns explicitly mentioned:"]},{"cell_type":"code","metadata":{"id":"1RLYPF_gibMN"},"source":["df.groupby(['key1', 'key2'])[['data2']].mean()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"sjCLxeEa-qBK"},"source":["We can do the same for Series:"]},{"cell_type":"code","metadata":{"id":"jOIn79KpibMO"},"source":["s_grouped = df.groupby(['key1', 'key2'])['data2']\n","s_grouped"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5FeIrivB_McR"},"source":["So applying an operation on it will materialize a Series:"]},{"cell_type":"code","metadata":{"id":"Oy9rOh3mibMO"},"source":["s_grouped.mean()"],"execution_count":null,"outputs":[]}]}