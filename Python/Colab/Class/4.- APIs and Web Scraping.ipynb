{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"},"colab":{"name":"APIs and Web Scraping.ipynb","provenance":[{"file_id":"19xi0OW3o60U9hg7vmxPdK7b0G4VUtcfY","timestamp":1611067740218},{"file_id":"1Dj-9XRfzse9cTErcEzVJWn3NCV-n7p4d","timestamp":1608709244433}],"collapsed_sections":["xqc9FvfOic9d","Sgfy9X-2fQBp","BIE81VvWgJd8","WWifxG-8ic9n","jH7bVtRfic9u","seJZ2sUJic9u","VmZCMht_ic9u"],"toc_visible":true}},"cells":[{"cell_type":"markdown","metadata":{"id":"SqwRXJdjic9c"},"source":["# Introduction to Data Analysis with Python III: APIs and Web Scraping\n","\n","<img src=\"https://www.python.org/static/img/python-logo.png\" alt=\"Python\" style=\"width: 200px; float: right;\"/>"]},{"cell_type":"markdown","metadata":{"id":"xqc9FvfOic9d"},"source":["#  Web APIs\n","\n"]},{"cell_type":"markdown","metadata":{"id":"l0Xi-cStris2"},"source":["An API (Aplication Programming Interface), is the way programs communicate with one another. Web APIs are the way programs communicate with one another _over the internet_.\n","\n","[RESTful](https://en.wikipedia.org/wiki/Representational_state_transfer) APIs respect a series of design principles that make them simple to use.\n","\n","The basic tools we are going to use are:\n","- POST and GET requests to urls we'll specify\n","- JSON objects that we'll receive as response or send as payload (in a POST command, for example).\n","\n","We'll use the handy Python module [`requests`](https://requests.readthedocs.io/en/master/):"]},{"cell_type":"code","metadata":{"id":"KSkByKueic9e"},"source":["import requests\n","\n","resp = requests.get('http://www.elpais.com/')\n","resp.content[:500]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NLepimFYjx49"},"source":["type(resp.content)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"CxT0yI30kTWk"},"source":["resp.encoding"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"AemgGDODkwfE"},"source":["resp.text"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Sgfy9X-2fQBp"},"source":["## On dealing with HTTP requests in Python\n","\n","urllib originates in Python 2, in Python 3 has been rewritten an it's the part of the standard Python library that deals with HTTP. Additionally, there's a urllib 3 package that, despite its name, is not related to the Python HTTP standard library as urllib. The package `requests` is based internally on `urllib3` but aims for an easier to use API than `urllib` or `urllib3`."]},{"cell_type":"code","metadata":{"id":"JsQsoxbbfZ2Z"},"source":["from urllib.request import urlopen\n","\n","html = urlopen('http://pythonscraping.com/pages/page1.html')\n","print(html.read())"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"EToDbexLffA4"},"source":["This command outputs the complete HTML code for page1 located at the URL http://pythonscraping.com/pages/page1.html. More accurately, this outputs the HTML file page1.html, found in the directory <web root>/pages, on the server located at the domain name http://pythonscraping.com."]},{"cell_type":"markdown","metadata":{"id":"BIE81VvWgJd8"},"source":["## Connecting reliably and handling exceptions\n","\n","The web is messy. When fetching a file, two main things can go wrong:\n","\n","- The page is not found on the server (or there was an error in retrieving it).\n","- The server is not found.\n","\n","In the first situation, an HTTP error will be returned. This HTTP error may be “404 Page Not Found,” “500 Internal Server Error,” and so forth. In all of these cases, the urlopen function will throw the generic exception HTTPError. "]},{"cell_type":"markdown","metadata":{"id":"UiB6RSCYgTtF"},"source":["You can handle this exception in the following way using `urllib`:"]},{"cell_type":"code","metadata":{"id":"zWLLsEZtgVsV"},"source":["from urllib.request import urlopen\n","from urllib.error import HTTPError\n","\n","try:\n","    html = urlopen('http://www.pythonscraping.com/pages/page1.html')\n","except HTTPError as e:\n","    print(e)\n","    # return null, break, or do some other \"Plan B\"\n","else:\n","    # program continues. Note: If you return or break in the  \n","    # exception catch, you do not need to use the \"else\" statement"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GVJxaSlYgY0u"},"source":["Or like this if your're using the `requests` package:"]},{"cell_type":"code","metadata":{"id":"-jMhuaNpgcoE"},"source":["import requests\n","\n","try:\n","  html = requests.get('http://www.pythonscraping.com/pages/page1.html')\n","except requests.exceptions.RequestException as e:\n","    raise SystemExit(e)\n","else:\n","  # Program continues"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ULS072mJgfih"},"source":["If you want http errors (e.g. 401 Unauthorized) to raise exceptions, you can call `Response.raise_for_status`. That will raise an `HTTPError`, if the response was an http error.\n","\n","An example:"]},{"cell_type":"code","metadata":{"id":"xY4F9FYEgixw"},"source":["try:\n","    r = requests.get('http://www.google.com/nothere')\n","    r.raise_for_status()\n","except requests.exceptions.HTTPError as err:\n","    raise SystemExit(err)\n","else:\n","  # Program continues"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"h8cseGrUic9f"},"source":["## Gettting data from International Space Station orbit\n","\n","This is an API that returns the current position of the ISS:"]},{"cell_type":"code","metadata":{"id":"jna2qRfHic9g"},"source":["r = requests.get('http://api.open-notify.org/iss-now.json')\n","r.status_code"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wOSzw9SHic9h"},"source":["r.content"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ubOdQtxNic9i"},"source":["We can convert a json-formatted string such as the one we get in the response into a Python object with the json library:"]},{"cell_type":"code","metadata":{"id":"ZDBIK7vric9j"},"source":["import json \n","\n","pos = json.loads(r.content)\n","pos"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_3wenZ-tlKtd"},"source":["Instead of the package JSON, there's the `json()` method in the request package that allows us to do the same:"]},{"cell_type":"code","metadata":{"id":"GpZO4FNalVT8"},"source":["r.json()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0ScBp-FaJeN8"},"source":["We can see that the `pos` object before returned by `json.loads()` on the returned content is a dictionary as well:"]},{"cell_type":"code","metadata":{"id":"6z922ztBlc7w"},"source":["type(pos)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-glNK9u0lX5Y"},"source":["Once we've got the our data parsed into a Python dictionary, accessing parts of it should be familiar:"]},{"cell_type":"code","metadata":{"id":"npND1aVpic9k"},"source":["pos['iss_position']['latitude']\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WsjXeCcWl6mW"},"source":["We can use Pandas to directly import the result of the request (JSON) in to a dataframe object:"]},{"cell_type":"code","metadata":{"id":"pJs8A_OIic9l"},"source":["import pandas as pd\n","\n","pd.read_json('http://api.open-notify.org/iss-now.json')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3EI6F26Bic9l"},"source":["We also can go in the other direction and generate json-formatted strings from Python objects:"]},{"cell_type":"code","metadata":{"id":"ed5lomOVic9m"},"source":["mi_diccionario = {'Chicago' : \"Illinois\", \"Kansas City\" : [\"Kansas\", \"Missouri\"]}"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HQO3U0g4ic9m"},"source":["mi_diccionario"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lmg7YBZRic9m"},"source":["json.dumps(mi_diccionario)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WWifxG-8ic9n"},"source":["### Exercise"]},{"cell_type":"markdown","metadata":{"id":"IgC4hJ6jrxtj"},"source":["\n","Write a function that returns the duration of the next 5 overhead passes of the ISS for a given latitude and longitude. Use http://open-notify.org/Open-Notify-API/ISS-Pass-Times/\n",". We are going to need to encode the parameters in the url as per the specification.\n","\n","For example, for Madrid the URL would be http://api.open-notify.org/iss-pass.json?lat=40.4&lon=-3.7&n=5"]},{"cell_type":"markdown","metadata":{"id":"9wS91_FlwWlA"},"source":["Let's work our way towards building a solution by using iPython quick feedback loop:"]},{"cell_type":"code","metadata":{"id":"bcpIwyZGxSII"},"source":["import sys\n","sys.version_info"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qCOgTjitic9n"},"source":["positions = requests.get(\"http://api.open-notify.org/iss-pass.json?lat=40.4&lon=-3.7&n=5\")\n","positions.text"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kv-cQQgCwerv"},"source":["Looks like this is a JSON-formatted document, just as we were expecting by the looks of the URL we're calling:"]},{"cell_type":"code","metadata":{"id":"eH3vuIGNwGcs"},"source":["positions.json()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"723PRNXtJxmF"},"source":["Once we've explored a bit and tested what we wanted to do, let's write a function to encapsulate it:"]},{"cell_type":"code","metadata":{"id":"sE8z0jsYic9n"},"source":["def get_iss(lat, lon, passes):\n","    \n","    url = f\"http://api.open-notify.org/iss-pass.json?lat={lat}&lon={lon}&n={passes}\"\n","    response = requests.get(url)\n","    result =response.json()\n","    \n","    return result['response']\n","\n","get_iss(40.0, 3.5, 5)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"EXUvNp62ic9n"},"source":["Although we managed to get the response, more complicated sets of parameters will be a complicated and error-prone thing to encode. Thankfully, the `requests` library can do that work for us, allowing us to pass a dictionary storing all the parameters:"]},{"cell_type":"code","metadata":{"id":"NwxbJAm2ic9o"},"source":["madrid_coords = {'lat': 40.4, 'lon': -3.7}\n","\n","r = requests.get('http://api.open-notify.org/iss-pass.json', params=madrid_coords)\n","r.json()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QvvJFqgtic9o"},"source":["resp = r.json()['response']\n","\n","pd.DataFrame(resp)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_iz2F8wCic9p"},"source":["Even more complicated sets of parameters are sometimes required. When that is the case, API designers often decide to require them in json format, received via a `POST` request.\n","\n","For example, take a look at the [QPX api from Google](https://developers.google.com/qpx-express/v1/trips/search). In the documentation, they define the body of the request, which we will have to provide, and of the response, which they'll provide back."]},{"cell_type":"code","metadata":{"id":"XvECjY61ic9p"},"source":["help(requests.post)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"yCiJ6aAzic9q"},"source":["# Web scraping\n","\n","**TODO** Intro to the different kinds of web: public, deep and dark. Why scraping and crawling."]},{"cell_type":"markdown","metadata":{"id":"r1mFOVQXoQkP"},"source":["\n","Let's now see how to use Python to request information from a web server, how to perform basic handling of the server’s response, and how to begin interacting with a website in an automated fashion. \n","\n","Let's now see how an HTML document looks like and how the DOM (Dcoument Object Model) associated to it could loook like as well:\n","\n","![HTML to DOM](http://www.cs.toronto.edu/~shiva/cscb07/img/dom/treeStructure.png)\n","\n","![DOM TREE](http://www.openbookproject.net/tutorials/getdown/css/images/lesson4/HTMLDOMTree.png)\n","\n","The DOM represents the document as nodes and objects. It's used by browsers and programming languages to connect to that page. HTML will generate a DOM, but the latter can be modified by the browser and Javascript, so sometimes the relation HTML/DOM is not going to be biunivocal."]},{"cell_type":"markdown","metadata":{"id":"ilxtCCEZlOi8"},"source":["We've got different libraries to tackle web scraping with Python:\n","\n","- Beautiful Soup (leveraging Request)\n","- Scrapy\n","- Selenium\n","\n","Scrapy is a complete web scraping framework which takes care of everything from getting the HTML, to processing the data. Selenium is a browser automation tool that can for example enable you to navigate between multiple pages. These two libraries have a steeper learning curve than Request (used to get HTML data) combined with BeautifulSoup which is used as a parser for the HTML. So, we'll start using BeautifulSoup and we'll learn a bit about Selenium later in this notebook."]},{"cell_type":"code","metadata":{"id":"aCaM4TQm746S"},"source":["la_url = 'https://aflcio.org/what-unions-do/social-economic-justice/advocacy/legislative-alerts'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"uhTLh-w2ic9q"},"source":["from IPython.display import display, HTML\n","\n","display(HTML(la_url))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"PqPMutD38M6o"},"source":["This will work on Jupyter, but won't on Colab:"]},{"cell_type":"code","metadata":{"id":"3J4JzxQ173Bx"},"source":["from IPython.display import IFrame\n","\n","IFrame(la_url, 800, 600)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"TxuAlI2Ol8lk"},"source":["Let's now use the Beautiful Soup library, named after Lewis Carrol's Alice in Wonderland Beautiful Soup poem (because its goal is to make sense of the nonsensical). Typically, you will need to install this library using `pip`, but in the case of Google Colab it's already included for you:"]},{"cell_type":"code","metadata":{"id":"-Y20tpktic9q"},"source":["from bs4 import BeautifulSoup\n","\n","r = requests.get(la_url)\n","\n","page = r.content\n","page[:1000]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"glMy9RzHmOil"},"source":["Let's talk about the parsers Beautiful Soup can use.\n","\n","`html.parser` is a parser that is included with Python 3 and requires no extra installations in order to use. It is reasonably fast.\n","\n","Another popular HTML parser is `html5lib`, that we're going to use here. `html5lib` is an extremely forgiving parser that takes even more initiative correcting broken HTML. It also depends on an external dependency, and is slower than the `html.parser`. Despite this, it may be a good choice if you are working with messy or handwritten HTML sites.\n","\n","There's also the `lxml` parser. It is as well more forgiving with broken HTML than `html.parser` (but not so autocorrecting as `html5lib`), and is the fastest of the three. Depending of what you'll be parsing and your goals, you can choose one or the other."]},{"cell_type":"code","metadata":{"id":"ZC9ZFTz-ic9r"},"source":["soup = BeautifulSoup(page, 'html5lib')\n","print(soup.prettify()[:1000])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"oWo1Rx0Nic9r"},"source":["print(soup.prettify()[28700:30500])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MacfAUbEic9r"},"source":["help(soup.find_all)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ApHGCNEwic9s"},"source":["alerts = soup.find_all('div', class_='content-details')\n","print(len(alerts))\n","type(alerts)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LpP8lx_2-6iI"},"source":["ResultSet class is a subclass of a list. Looping through the results of find_all() is the most common approach to process the ResultSet as we'll see some cells down.\n","\n","Being a list-like structure, we can access the elements using the index:\n"]},{"cell_type":"code","metadata":{"id":"U1jc9TKAic9s"},"source":["alerts[0]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"J4o7qCUX_Usk"},"source":["Let's process this first alert a bit more and extract exactly the information we want from the HTML:"]},{"cell_type":"code","metadata":{"id":"t4ZBHVIuic9s"},"source":["first = alerts[0]\n","print(first.find('time').get_text())\n","print(first.a.find('span').get_text())\n","print(first.a['href'])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_Cf2kWV7_i_s"},"source":["Now, let's try to encapsulate this logic into a function that extracts all the information we want from the alerts in the page.\n","\n","If you're curious, we're using Python type hints to tell our tooling that we're returning a list."]},{"cell_type":"code","metadata":{"id":"2QgMDhOnic9s"},"source":["import typing\n","\n","# Python 3.9 onwards, you can use list[dict] instead of typing.List[dict]\n","def get_aflcio_alerts() -> typing.List[dict]:\n","    # Initialize the results list to push elements when ready\n","    result = []\n","    r = requests.get('http://www.aflcio.org/Legislation-and-Politics/Legislative-Alerts')\n","    soup = BeautifulSoup(r.content, 'html5lib')\n","    \n","    for alert in soup.find_all('div', class_='content-details'):\n","        # Initialize a dictionary to store our information\n","        dictionary = {}\n","        dictionary['date'] = alert.find('time').get_text()\n","        dictionary['title'] = alert.a.find('span').get_text()\n","        dictionary['link'] = 'http://www.aflcio.org' + alert.a['href']\n","        \n","        # Add the alert data to our list\n","        result.append(dictionary)\n","        \n","    return result"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HvflDC--BSpZ"},"source":["Ok, let's test our function and show some results:"]},{"cell_type":"code","metadata":{"id":"MBQWlc23ic9t"},"source":["letters = get_aflcio_alerts()\n","letters[:2]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lXrXPH9DEEbZ"},"source":["And we come full circle! We encode the list we created in a json string. We could then provide that over the internet in our own API (something that is out of the scope of this class, but very interesting nevertheless)."]},{"cell_type":"code","metadata":{"id":"2hRSI44Pic9t"},"source":["json.dumps(letters)[:1000]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Jnt8sEAjnHkS"},"source":["## Practice - Scraping IMDB movie data\n","\n","**TODO** https://www.dataquest.io/blog/web-scraping-beautifulsoup/"]},{"cell_type":"markdown","metadata":{"id":"v0s9G-Ieic9t"},"source":["## Ultra easy scraping with pandas!\n","\n","When the data we want is already formatted as a table, we can do it even more easily! Just use `pandas.read_html`:"]},{"cell_type":"code","metadata":{"id":"JCmIbLaLic9t"},"source":["tables = pd.read_html('https://en.wikipedia.org/wiki/List_of_accidents_and_disasters_by_death_toll', header=0)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QafmC3-mic9t"},"source":["tables[4].head()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MQZ3s1aIg1Qn"},"source":["## Scraping with Selenium\n","\n","Selenium is an open source tool to automate testing in web browsers. It does not need to use a full fledged browser, we can use it with what's called a \"headless\" browser, as we'll be doing here.\n","\n","A *headless* browser is just a regular browser that contains no visible UI elements. It can do way more than make requests, it can also render HTML though you will not see it, keep session information and even perform asynchronous network communications running Javascript code. Headless browsers are essential for any automation task.\n","\n","To use Selenium with a browser, you need to install a WebDriver for the browser of your choosing. Also, you'd need to install the Selenium Python package (Selenium exists independently of Python, but a module is available to use Selenium inside Python).\n","\n","Instead of going through the installation of all this in your machine, we'll be taking advantage of a ready made Python module that imports the Chromium driver to be used inside Colab notebooks. Worry not, Selenium installation is outside the scope of this class, but if you're interested there's plenty of information in the Internet about it."]},{"cell_type":"markdown","metadata":{"id":"yPSlm0N512Vg"},"source":["Let's practice with Selenium over a scrape-friendly (licensing wise) site that lists some books. The site is [Books to scrape](http://books.toscrape.com/).\n","\n","We'll scrape the details of each book on the page. Each page has 20 books and the details of the book can be found using the URL on each book's card. We'll do this for all the books in the page and for all the page the site has."]},{"cell_type":"markdown","metadata":{"id":"y0qY04Ra2roz"},"source":["Pages follow a simple URL structure, and we'll use that. If the site you're scraping needs a button to be clicked, Selenium can do that as well.\n","\n","\n","### Scraping one book\n","\n","Open [Chrome Developer Tools](https://developers.google.com/web/tools/chrome-devtools?hl=es) and navigate one of the books, centering around the `<article class=\"product_pod\">` element. This HML code contains the URL of the book detail, that in turn contains what we want to scrap:\n","\n","- Title\n","- Stock Status\n","- Rating\n","- Description\n","- Price\n","- Tax\n","- UPC\n","\n","The first thing we need to do is install/import our webdriver and the Selenium Python package, as mentioned before. We'll be using the [Kora](https://github.com/airesearch-in-th/kora) package that will simplify our task here:"]},{"cell_type":"code","metadata":{"id":"SDSyQqci4eaw"},"source":["!pip install kora -q"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MFbmCfMPnTf4"},"source":["Now, let's import the Chromium webdriver and use it to load the first page of the Site we want to scrape:"]},{"cell_type":"code","metadata":{"id":"WWl7SH6i4oa5"},"source":["from kora.selenium import wd\n","wd.get('http://books.toscrape.com/catalogue/category/books_1/page-1.html')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"r3Au-KbjneCK"},"source":["Inspecting the page with Chrome Dev Tools, we can see that we get the detailed book information from the book URL that's in the element `product_pod`. Let's use the webdriver to find all the elements in the page and selectd the first one:"]},{"cell_type":"code","metadata":{"id":"RuGJEZCm5iz6"},"source":["product_pod = wd.find_elements_by_class_name(\"product_pod\")[0]\n","product_pod"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pvIh6WxGnwnV"},"source":["Got it! Now, let's dig into `product_pod` structure to extract the link we're interested in. Once more, what we're looking for is outlined in the source code of the page that we can navigate using Chrome Dev Tools:"]},{"cell_type":"code","metadata":{"id":"hqfaAS046O0l"},"source":["book_link = product_pod.find_element_by_tag_name(\"h3\").find_element_by_tag_name(\"a\").get_property(\"href\")\n","book_link"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_nHckA8boGt1"},"source":["Now we'll use again the webdriver to load the book description page and continue our analysis so we can scrape the data we're interested in:"]},{"cell_type":"code","metadata":{"id":"mlAuDTwi6rLS"},"source":["wd.get(book_link)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Ia03NdeboRj2"},"source":["We'll navigate the document structure using `find_element_by_xpath`. XPath is convenient because it's quite powerful when you don’t have a suitable id or name attribute for the element you wish to locate. You can use XPath to either locate the element in absolute terms (not recommended), or relative to an element that does have an id or name attribute.\n","\n","Use this [tutorial](http://www.zvon.org/comp/r/tut-XPath_1.html#Pages~List_of_XPaths) to understand the XPath syntax and different wildcards that we're using here. Once we get what we want"]},{"cell_type":"code","metadata":{"id":"eAnJAwBu6yY5"},"source":["title = wd.find_element_by_xpath(\"//*[@id='content_inner']/article/div[1]/div[2]/h1\")\n","price = wd.find_element_by_xpath(\"//*[@id='content_inner']/article/div[1]/div[2]/p[1]\")\n","stock_status = wd.find_element_by_xpath(\"//*[@id='content_inner']/article/div[1]/div[2]/p[2]\")\n","rating = wd.find_element_by_xpath(\"//*[@id='content_inner']/article/div[1]/div[2]/p[3]\").get_attribute(\"class\")\n","\n","description = wd.find_element_by_xpath(\"//*[@id='content_inner']/article/p\")\n","upc = wd.find_element_by_xpath(\"//*[@id='content_inner']/article/table/tbody/tr[1]/td\")\n","tax = wd.find_element_by_xpath(\"//*[@id='content_inner']/article/table/tbody/tr[5]/td\")\n","category =  wd.find_element_by_xpath(\"//*[@id='default']/div/div/ul/li[3]/a\")\n","\n","print(f\"Title: {title}\\n\",\n","      f\"Description: {description}\\n\",\n","      f\"Rating: {rating}\\n\",\n","      f\"Stock Status: {stock_status}\\n\",\n","      f\"Price: {price}\\n\",\n","      f\"Tax: {tax}\\n\",\n","      f\"UPC: {upc}\\n\"\n","      )"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-ImkEVccqLYb"},"source":["Let's clean this up a bit by accessing the text inside the element in most of the attributes we want:"]},{"cell_type":"code","metadata":{"id":"iBaDmk29pTaL"},"source":["\n","book = {\n","    'Title': title.text,\n","    'Description': description.text,\n","    'Rating': rating,\n","    'Stock Status': stock_status,\n","    'Price': price.text,\n","    'Tax': tax.text,\n","    'UPC': upc.text\n","}\n","\n","book"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6AqUD8Hsyg_S"},"source":["Ok, it looks like we almost got our book dictionary, but there are still some rough edges. Let's focus first on the 'Stock status' property, and try to extract the number of items in stock from it:"]},{"cell_type":"code","metadata":{"id":"ZDBVAjX-xbWq"},"source":["import re\n","\n","book['Stock Status'] = int(re.findall(\"\\d+\",stock_status.text)[0])\n","\n","book"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dofuyoVAy03J"},"source":["Ok, now that we're all set, let's clen up the rating a bit so it's a number we can work with. We'll extract the number name and use a module to convert the name to an integer:"]},{"cell_type":"code","metadata":{"id":"jP7xOBrNyID8"},"source":["!pip install word2number"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"TVJEKfpjx4W_"},"source":["from word2number import w2n\n","book['Rating'] = w2n.word_to_num(rating.split()[1])\n","\n","book"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KtmohGP71H5u"},"source":["Let's wrap all this book scraping logic into a Python function, so we can iterate easily over all the books in the page:"]},{"cell_type":"code","metadata":{"id":"kzpIP_xvzgA5"},"source":["def scrape_book(book_link):\n","  wd.get(book_link)\n","  book = {\n","    'Title': wd.find_element_by_xpath(\"//*[@id='content_inner']/article/div[1]/div[2]/h1\").text,\n","    'Description': wd.find_element_by_xpath(\"//*[@id='content_inner']/article/p\").text,\n","    'Rating': w2n.word_to_num(wd.find_element_by_xpath(\"//*[@id='content_inner']/article/div[1]/div[2]/p[3]\").get_attribute(\"class\").split()[1]),\n","    'Stock Status': int(re.findall(\"\\d+\",wd.find_element_by_xpath(\"//*[@id='content_inner']/article/div[1]/div[2]/p[1]\").text)[0]),\n","    'Price': wd.find_element_by_xpath(\"//*[@id='content_inner']/article/div[1]/div[2]/p[1]\").text,\n","    'Tax': wd.find_element_by_xpath(\"//*[@id='content_inner']/article/table/tbody/tr[5]/td\").text,\n","    'UPC': wd.find_element_by_xpath(\"//*[@id='content_inner']/article/table/tbody/tr[1]/td\").text\n","  }\n","  return book"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"M0BkMxvc1k6g"},"source":["scrape_book(book_link)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DcZc0DbB37xm"},"source":["### Scraping all the books in one page"]},{"cell_type":"markdown","metadata":{"id":"ERaLWfYmqkbU"},"source":["Ok, time to get all the books in the first page. First, let's reset what the webdriver point to by reloading the first page of books:"]},{"cell_type":"code","metadata":{"id":"nyjFXNO04qm-"},"source":["wd.get('http://books.toscrape.com/catalogue/category/books_1/page-1.html')\n","wd"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"EIb5V5gwquai"},"source":["Let's find all the `product_pod` elements in the page. This contains all the links we're interested in:"]},{"cell_type":"code","metadata":{"id":"kT0Ll7cA5tf8"},"source":["product_pods = wd.find_elements_by_class_name(\"product_pod\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jkc5Lpceq1PW"},"source":["This is a list:"]},{"cell_type":"code","metadata":{"id":"vw0W-Buv68XK"},"source":["type(product_pods)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-kukhpXbq3ZV"},"source":["...but the elements are not `string`. Be careful with this as you won't be able to manipulate what the webdriver points to easily when iterating:"]},{"cell_type":"code","metadata":{"id":"1ExZP_ox6_Yu"},"source":["type(product_pods[0])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ofXgPykKrNho"},"source":["We'll need to access the books link in order to carefully build a list of dictionaries containing the book info we want to scrape:"]},{"cell_type":"code","metadata":{"id":"av7Yk32D56Os"},"source":["book_links = []\n","for product_pod in product_pods:\n","  book_link = product_pod.find_element_by_tag_name(\"h3\").find_element_by_tag_name(\"a\").get_property(\"href\")\n","  book_links.append(book_link)\n","\n","books=[]\n","for book_link in book_links:\n","  books.append(scrape_book(book_link))\n","\n","books"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dFa-Wgov4HcV"},"source":["Again, let's wrap this up in a nice Python function:"]},{"cell_type":"code","metadata":{"id":"91aHSVYU4MBa"},"source":["def scrape_page(page_link):\n","  wd.get(page_link)\n","  \n","  book_links = []\n","  for product_pod in product_pods:\n","    book_link = product_pod.find_element_by_tag_name(\"h3\").find_element_by_tag_name(\"a\").get_property(\"href\")\n","    book_links.append(book_link)\n","\n","  books=[]\n","  for book_link in book_links:\n","    books.append(scrape_book(wd, book_link))\n","\n","  return books"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cUXtacTbLMZv"},"source":["Now with this new function you could scrape the whole site. Try to reuse this function and do it as an exercise.\n","\n","To end using Selenium, let's close the webdriver so we don't keep the headless browser open:"]},{"cell_type":"code","metadata":{"id":"dw7HCuNvqgz_"},"source":["wd.close()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jH7bVtRfic9u"},"source":["# Exercises"]},{"cell_type":"markdown","metadata":{"id":"seJZ2sUJic9u"},"source":["### Exercise:\n","\n","Extract the date of the worst aviation disaster from: https://en.wikipedia.org/wiki/List_of_accidents_and_disasters_by_death_toll\n","\n","Prerequisites: pandas, pd.read_html"]},{"cell_type":"code","metadata":{"id":"oWo9K8LFic9u"},"source":["aviation = pd.read_html('https://en.wikipedia.org/wiki/List_of_accidents_and_disasters_by_death_toll', header=0)[1]\n","aviation.head(1)['Date']"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VmZCMht_ic9u"},"source":["### Exercise: \n","\n","Assuming the list is exhaustive, calculate how many people died in accidental explosions per decade in the XX century. Plot it.\n","\n","Data: \n","https://en.wikipedia.org/wiki/List_of_accidents_and_disasters_by_death_toll\n","\n","Prerequisites: pandas, pd.read_html, pd.to_datetime, matplotlib or seaborn"]},{"cell_type":"code","metadata":{"id":"RF0CZWpVic9u"},"source":["explosions = pd.read_html('https://en.wikipedia.org/wiki/List_of_accidents_and_disasters_by_death_toll', header=0)[4]\n","explosions.head()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9C0aD2DBic9v"},"source":["explosions['year'] = explosions['Date'].str[-4:]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gg1TBmNlic9w"},"source":["explosions['Deaths'] = pd.to_numeric(explosions.Deaths.str.replace('[^0-9]', ''))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"nNTbWZzcic9w"},"source":["explosions['Decade'] = explosions['year'].str[:3] + '0s'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2HzFrPbpSkqX"},"source":["explosions.head()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0XVaqVpbic9w"},"source":["twentieth_century = explosions[(explosions['date'] > '1900') & (explosions['date'] < '2000')]\n","per_decade = twentieth_century.groupby('Decade')['Deaths'].sum()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ljTpwNAITtMP"},"source":["per_decade"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YU2sqEz2ic9x"},"source":["import seaborn as sns\n","%matplotlib inline\n","\n","sns.barplot(data=twentieth_century, \n","            x='Decade', \n","            y='Deaths', \n","            order=sorted(twentieth_century['Decade'].unique()),\n","            ci=None,\n","            color='darkgrey')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9HT6NTKpic9x"},"source":["### Exercise: \n","\n","create a function that, given the two tables extracted from http://en.wikipedia.org/wiki/List_of_S%26P_500_companies and a date, returns the list of companies in the S&P 500 at that date."]}]}