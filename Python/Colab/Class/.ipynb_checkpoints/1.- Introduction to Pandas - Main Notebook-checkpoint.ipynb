{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uZpQ4BDTppii"
   },
   "source": [
    "# Introduction to Data Analysis with Python\n",
    "\n",
    "\n",
    "<img src=\"https://www.python.org/static/img/python-logo.png\" alt=\"yogen\" style=\"width: 200px; float: right;\"/>\n",
    "<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "id": "PWoNoXiuppii"
   },
   "source": [
    "# Objectives\n",
    "\n",
    "* Handle tabular data with `pandas`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z9XzZz55ppii"
   },
   "source": [
    "# The Python scientific stack: SciPy\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l3qpvAHUvun6"
   },
   "source": [
    "Python Main Data Libraries\n",
    "NumPy: Base N-dimensional array package\n",
    "\n",
    "SciPy library: Fundamental library for scientific computing\n",
    "\n",
    "Matplotlib: Comprehensive 2D Plotting\n",
    "\n",
    "IPython: Enhanced Interactive Console\n",
    "\n",
    "Sympy: Symbolic mathematics\n",
    "\n",
    "pandas: Data structures & analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oNMknaq3Lgu6"
   },
   "source": [
    "## Saving our work to Google Drive\n",
    "\n",
    "Colab is a very convenient environment, but an ephemeral one. Let's first configure our notebook to save our work to Google Drive, where it can be persisted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yxfzsiIyLw21"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "drive_loc = '/content/gdrive'\n",
    "files_loc = os.path.join(drive_loc, 'MyDrive', 'pdsfiles')\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount(drive_loc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zgCjEepDJe9U"
   },
   "source": [
    "Let's create a directory to hold all our work and make sure it's there:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AbqMNDZuL5bF"
   },
   "outputs": [],
   "source": [
    "!mkdir -p {files_loc}\n",
    "!ls {files_loc}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MabkSjlippij"
   },
   "source": [
    "## `pandas`\n",
    "\n",
    "Distinct set of requirements that pandas introduces:\n",
    "\n",
    "- Data structures with labeled axes supporting automatic or explicit data alignment. This prevents common errors resulting from misaligned data and working with differently-indexed data coming from different sources.\n",
    "\n",
    "- Integrated time series functionality.\n",
    "\n",
    "- The same data structures handle both time series data and non-time series data.\n",
    "\n",
    "- Arithmetic operations and reductions (like summing across an axis) would pass on the metadata (axis labels).\n",
    "\n",
    "- Flexible handling of missing data.\n",
    "\n",
    "- Merge and other relational operations found in popular database databases (SQL-based, for example)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "98AeC3GUppij"
   },
   "source": [
    "### Getting started with pandas\n",
    "\n",
    "First, always remember to check the [API Reference](https://pandas.pydata.org/pandas-docs/stable/reference/index.html).\n",
    "\n",
    "![Read the effing manual](https://i.kym-cdn.com/photos/images/newsfeed/000/017/668/Mao_RTFM_vectorize_by_cmenghi.png?1318992465)\n",
    "\n",
    "Convention: when you see `pd`, it means the imported `Pandas` module. We'll be combining it with Numpy as well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2zoyhRZLppij"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XcuaZAy3J67c"
   },
   "source": [
    "Pandas is evolving. If you look around in the Internet or articles/books, you may find things that go back to Pandas 0.21. In our case, in the included Colab supported runtime, we've got:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Pz4PQcFhGmyG"
   },
   "outputs": [],
   "source": [
    "pd.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xrwTa-uUKQxS"
   },
   "source": [
    "...which is quite updated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5gPMihG2ppij"
   },
   "source": [
    "Pandas Data Structures are **Series** and **Dataframes**. While they are not a universal solution, they provide a solid, easy-to-use foundation to data mangling tooling in the Data Science world."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_4r39ua3ppij"
   },
   "source": [
    "### Series\n",
    "\n",
    "A Series is a one-dimensional array-like object containing\n",
    "- an array of **data** (of any NumPy data type)\n",
    "- an associated array of data labels, called its **index**.\n",
    "\n",
    "It is the base pandas abstraction. You can thing of it as the love child of a numpy array and a dictionary, sort of-ish.\n",
    "\n",
    "The simplest Series is formed from only an array of data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6GNZQWjMppij"
   },
   "outputs": [],
   "source": [
    "s = pd.Series([4, 7, -5, 3])\n",
    "type(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d3QzcTOJ4buz"
   },
   "outputs": [],
   "source": [
    "s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i8Q7MYI5ppij"
   },
   "source": [
    "The string representation of a Series displayed interactively shows the index on the left and the values on the right. Since we did not specify an index for the data, a default one consisting of the integers 0 through N - 1 (where N is the length of the data) is created.\n",
    "\n",
    "If we provide an index, pandas will use it. If not, it will automatically create one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "51gsf70n6We9"
   },
   "outputs": [],
   "source": [
    "s.index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5hBVtntzK3g5"
   },
   "source": [
    "Yep, that was Pandas [optimizing things](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.RangeIndex.html#:~:text=RangeIndex%20is%20a%20memory%2Dsaving,is%20provided%20by%20the%20user.). Same thing, we can ask our Series for just the values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fwdy0j1Ippij"
   },
   "outputs": [],
   "source": [
    "s.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j4z5kjV1LXlG"
   },
   "source": [
    "Just to be sure, I told you this was a Numpy array. It is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nDYNYgTrLUSg"
   },
   "outputs": [],
   "source": [
    "type(s.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1MJA_qSR6u10"
   },
   "source": [
    "Often it will be desirable to create a Series with an index identifying each data point:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Fiepzbn5ppij"
   },
   "outputs": [],
   "source": [
    "s2 = pd.Series([1, 2, 4.5, 7, 2, 23, 15], index=list('javierc'))\n",
    "s2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lMTx_jAy7CP6"
   },
   "source": [
    "We were quite lazy back there providing the list, weren't we? \n",
    "\n",
    "Compared with a regular NumPy array, you can use values in the index when selecting single values or a set of values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U21_4l9Gppij"
   },
   "outputs": [],
   "source": [
    "s2['r']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "loN21f8q7OYU"
   },
   "source": [
    "Now, that's convenient.\n",
    "\n",
    "NumPy array operations, such as filtering with a boolean array, scalar multiplication, or applying math functions, that you now know better than me, will preserve the index-value link:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "279Gk6Adppij"
   },
   "outputs": [],
   "source": [
    "s2 % 2 == 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8OPC4IsAMG3y"
   },
   "source": [
    "We can similarly apply boolean selection operations to the Series: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5Ovmtw3mppik"
   },
   "outputs": [],
   "source": [
    "s2[s2 % 2 == 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8tqesChmMQ-h"
   },
   "source": [
    "...and go beyond pure logical operations, of course:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fLecZtU9ppik"
   },
   "outputs": [],
   "source": [
    "s2 * 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GeU2Opoippik"
   },
   "outputs": [],
   "source": [
    "np.exp(s2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "br-nT4O8MfxC"
   },
   "source": [
    "Note that these operations are not being applied on the original object, but rather in a copy that's being returned:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C5uyCSKHMaUt"
   },
   "outputs": [],
   "source": [
    "s2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_Ou1PMK77mRa"
   },
   "source": [
    "Another way to think about a Series is as a fixed-length, ordered dict, as it is a mapping of index values to data values (because, remember, a Python dictionary is unordered, and it can be extended). It can be substituted into many functions that expect a dict:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Pu-rVCGappik"
   },
   "outputs": [],
   "source": [
    "'i' in s2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8lEVFAyvppik"
   },
   "source": [
    "We can create Series from dictionaries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rp-aesb2ppik"
   },
   "outputs": [],
   "source": [
    "sdata = {\n",
    "    'Zaragoza' : 2.5e5,\n",
    "    'Sevilla': 5e5,\n",
    "    'Cordoba': 3e5,\n",
    "    'Madrid': 6e6}\n",
    "\n",
    "s3 = pd.Series(sdata)\n",
    "s3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0NOsEetHrLs6"
   },
   "source": [
    "We'll be using s3 later on, so because of the ephemeral nature of Colab, we'll store our Series in our Google Drive instead of redoing all the steps.\n",
    "\n",
    "We'll be using as well a handy feature of Colab, which is [Colab Forms](https://colab.research.google.com/notebooks/forms.ipynb) to enable o disable the conditional execution of a code cell based on the UI input exposed by the form:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qUXYu0ScxAuc"
   },
   "outputs": [],
   "source": [
    "#@title Saving s3 to Drive\n",
    "import os\n",
    "save_to_drive = True #@param {type:\"boolean\"}\n",
    "\n",
    "if save_to_drive and files_loc:\n",
    "  s3.to_pickle(os.path.join(files_loc,\"s3.pkl\"))\n",
    "else:\n",
    "  print('Please, mount Google Drive running the cell at the beginning and try again')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_Tk4ANzHtbZp"
   },
   "source": [
    "Let's check that our pickle file is there:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O4msJKbUxTjD"
   },
   "outputs": [],
   "source": [
    "!ls {files_loc}/s3.pkl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y2NDOS8s9wWx"
   },
   "source": [
    "When only passing a dict, the index in the resulting Series will **not** have the dict’s keys in sorted order:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YIy3iQ5KubSu"
   },
   "outputs": [],
   "source": [
    "s3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7I7bCV30ud03"
   },
   "source": [
    "If you want the keys in sorted order, you need to explicitly define it with `sort_index()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vgeV9rG9uo90"
   },
   "outputs": [],
   "source": [
    "s3 = pd.Series(sdata).sort_index()\n",
    "s3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OEGFJp5Bu08V"
   },
   "source": [
    "You can control the ordering as well by explicitly defining the index order using a list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "--HBXL_x95Zj"
   },
   "outputs": [],
   "source": [
    "cities = ['Cordoba', 'Madrid', 'Valencia', 'Zaragoza']\n",
    "s4 = pd.Series(sdata, index=cities)\n",
    "s4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RSQtQfZQ_Uzm"
   },
   "source": [
    "In this case, 3 values found in sdata were placed in the appropriate locations, but since no value for 'Valencia' was found, it appears as NaN (not a number) which is considered in pandas to mark missing or NA values. We will use the terms “missing” or “NA” to refer to missing data. The isnull and notnull functions in pandas should be used to detect missing data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0OIXjMwf_g88"
   },
   "outputs": [],
   "source": [
    "pd.isnull(s4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lamIf-mx_or6"
   },
   "outputs": [],
   "source": [
    "pd.notnull(s4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xnqEc2aQBHVt"
   },
   "source": [
    "These can also be consumed as instance methods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xo2XvfciBOxU"
   },
   "outputs": [],
   "source": [
    "s4.notnull()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9Y0qFvbYBa9A"
   },
   "source": [
    "A critical `Series` feature for many applications is that it automatically aligns differently-indexed data in arithmetic operations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qSXX6fORppik"
   },
   "outputs": [],
   "source": [
    "s3 + s4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-7PapEEPppik"
   },
   "source": [
    "Both the Series object itself and its index have a name attribute, which integrates with other key areas of pandas functionality:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qQIKzJ1nppik"
   },
   "outputs": [],
   "source": [
    "s4.name = 'Population'\n",
    "s4.index.name = 'Province'\n",
    "s4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vhcQw4-gppik"
   },
   "outputs": [],
   "source": [
    "s3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hCBpaiPJVRE_"
   },
   "source": [
    "From what we've seen so far, it may look like the Series object is basically interchangeable with a one-dimensional NumPy array.\n",
    "\n",
    "The essential difference is the presence of the index: while the Numpy Array has an implicitly defined integer index used to access the values, the Pandas Series has an explicitly defined index associated with the values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HVk0Zm4cppik"
   },
   "source": [
    "### DataFrame\n",
    "\n",
    "A DataFrame represents a tabular, spreadsheet-like data structure containing an ordered collection of columns, each of which can be a different value type (numeric, string, boolean, etc.). The DataFrame has both a row and column index; it can be thought of as a dict of Series (one for all sharing the same index).\n",
    "\n",
    "Row-oriented and column-oriented operations in DataFrame are treated roughly symmetrically. Under the hood, the data is stored as one or more two-dimensional blocks rather than a list, dict, or some other collection of one-dimensional arrays.\n",
    "\n",
    "This is the object you'll work most of the time with. It represents a table of _m_ observations x _n_ variables. Each variable, or column, is a Series.\n",
    "\n",
    "There are numerous ways to construct a DataFrame, though one of the most common is from a dict of equal-length lists or NumPy arrays."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a1D4AdbRxqu0"
   },
   "source": [
    "Let's discuss the naming of axes in Pandas, both for series and dataframes.\n",
    "\n",
    "For Series, because is a one-dimensional array of values, we've got only **Axis 0**:\n",
    "\n",
    "![Series axes](https://drive.google.com/uc?export=view&id=1unXqkxjezJiaocs1D-OM2vdj9ad8OLbs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B-33mh2bz8w3"
   },
   "source": [
    "The dataframe, as we've just seen, is a two-dimensional structure. It has columns and row, colums made of separate Series objects. The axes in the dataframe are as follows, where if not explicitly mentioned, axis 0 will always make the default:\n",
    "\n",
    "![Dataframe axes](https://drive.google.com/uc?export=view&id=1f8jKqZTURUoM5wV1yz_Ax9PrXSr-uITy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aRf2KROr0wEm"
   },
   "source": [
    "Ok, enough theory. Let's go for some practice, defining a dataframe just as we said is quite common:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SNaVGVTsppik"
   },
   "outputs": [],
   "source": [
    "dfdata = {\n",
    "    'province' : ['M', 'M', 'M', 'B', 'B'],\n",
    "    'population': [1.5e6, 2e6, 3e6, 5e5, 1.5e6],\n",
    "    'year' : [1900, 1950, 2000, 1900, 2000]   \n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tOggFiROd9k7"
   },
   "source": [
    "The resulting DataFrame will have its index assigned automatically as with Series, and the columns (axis 1) are placed in sorted order:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WB-DcYHdd8Jw"
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(dfdata)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CMXrSBHIeYhS"
   },
   "source": [
    "If you specify a sequence of columns, the DataFrame’s columns will be exactly what you pass, and as with Series, if you pass a column that isn’t contained in data, it will appear with NA values in the result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2gfFchZfppik"
   },
   "outputs": [],
   "source": [
    "df2 = pd.DataFrame(dfdata, columns=['province','population', 'year', 'debt'])\n",
    "df2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k9z13SWh1MJR"
   },
   "source": [
    "If we check the nature of the index in the dataframe, we can see we have the same as in Series:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5U6lXaN4ppik"
   },
   "outputs": [],
   "source": [
    "df2.index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E8phvrtV1UfT"
   },
   "source": [
    "There's a new property we can access called `columns`, where we can see we've got just another index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qkGCxJGKppik"
   },
   "outputs": [],
   "source": [
    "df2.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AhWodD631eBk"
   },
   "outputs": [],
   "source": [
    "type(df2.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BkZAcLvNfF-R"
   },
   "source": [
    "A column in a DataFrame can be retrieved as a Series either by dict-like notation or by attribute:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-aFldgU7ppik"
   },
   "outputs": [],
   "source": [
    "df2['population']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JmJdr3-xppik"
   },
   "outputs": [],
   "source": [
    "df2.population"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nzed8BgXfOfu"
   },
   "source": [
    "Note that the returned Series have the same index as the DataFrame, and their name attribute has been appropriately set.\n",
    "\n",
    "You can see that what's being returned is in fact a `Series` object, altough as of now it should be quite clear:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J83sWKzefKJb"
   },
   "outputs": [],
   "source": [
    "type(df2.population)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rnUIgYPgf-HD"
   },
   "source": [
    "Using this notation, we can add more columns and they will be indexed following the criteria already defined by the dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4K30Lzvxppik"
   },
   "outputs": [],
   "source": [
    "df2['2nd_language'] = np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k4J6SG05gW9r"
   },
   "source": [
    "When assigning lists or arrays to a column, the value’s length must match the length of the DataFrame. If you assign a Series, it will be instead conformed exactly to the DataFrame’s index, inserting missing values in any holes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BwHAohw0ppik"
   },
   "outputs": [],
   "source": [
    "df2['2nd_language']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FgSOXMgFgHAB"
   },
   "source": [
    "But watch out with the naming used, you may hit some of the Python syntax constraints:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZTCjpvYzppik"
   },
   "outputs": [],
   "source": [
    "df2.2nd_language"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dBSDKQYQg8oe"
   },
   "source": [
    "As with Series, we can name the index of the dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bi6eNVE7ppil"
   },
   "outputs": [],
   "source": [
    "df2.index = list('abcde')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MC5Rvv2Sppil"
   },
   "outputs": [],
   "source": [
    "df2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vfOE3alw2KMs"
   },
   "source": [
    "This is not a RangeIndex anymore, but a regular Index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K6bbHQUv2Gur"
   },
   "outputs": [],
   "source": [
    "df2.index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hiVV2t8pcycY"
   },
   "source": [
    "We can access a particular row of the Dataframe using the property `loc`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_2zKJHVnppil"
   },
   "outputs": [],
   "source": [
    "df2.loc['c']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J7qjikqWeK1L"
   },
   "outputs": [],
   "source": [
    "type(df2.loc['c'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_JjpRQTMdkto"
   },
   "source": [
    "`loc` admits a list or array of labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ugzFu9JeiVhg"
   },
   "outputs": [],
   "source": [
    "df2.loc[['a', 'b']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gs8aZWfed73F"
   },
   "source": [
    "If we pass a list with the specific row instead of the label as is, we get the nice Dataframe formatting instead of the Series:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gNulUEFFipeV"
   },
   "outputs": [],
   "source": [
    "df2.loc[['c']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qRD7L5dUeQlr"
   },
   "outputs": [],
   "source": [
    "type(df2.loc[['c']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kve-6eZIeXhh"
   },
   "source": [
    "We can also pass it an slice object of labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "70kDfbNIixCx"
   },
   "outputs": [],
   "source": [
    "df2.loc['a':'c']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pveoPC8QephD"
   },
   "source": [
    "We can eve use a callable condition for matching rows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UWbhzsqPjUSq"
   },
   "outputs": [],
   "source": [
    "df2.loc[df2['year'] > 1950]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d9uMuWIHlnec"
   },
   "source": [
    "When assigning lists or arrays to a column, the value’s length must match the length of the DataFrame. If you assign a Series, it will be instead conformed exactly to the DataFrame’s index, inserting missing values in any holes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JXWo0zSrk1ks"
   },
   "outputs": [],
   "source": [
    "val = pd.Series([0.1, 0.6, 0.9], index=['b','d','e'])\n",
    "val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xVS52GfQfHk9"
   },
   "source": [
    "So let's define values for a particular label or column by passing them as a list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "27W50Ehhppil"
   },
   "outputs": [],
   "source": [
    "df2['debt'] = [1,0,2,.5,.7]\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vUJLqixslPeV"
   },
   "outputs": [],
   "source": [
    "df2['debt'] = val\n",
    "df2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-qSyKIdc6cjy"
   },
   "source": [
    "Assigning a column that doesn’t exist will create a new column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KPpySh-ippil",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df2['capital'] = df2['province'] == 'M'\n",
    "df2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qb_SGZIA7pXc"
   },
   "source": [
    "The del keyword will delete columns as with a dict:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l_6GA17j73A8"
   },
   "outputs": [],
   "source": [
    "del df2['2nd_language']\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5b03VZbunCug"
   },
   "outputs": [],
   "source": [
    "df2['2nd_language'] = np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VDysxPbN7DiG"
   },
   "source": [
    "You can always transpose the Dataframe and it will switch the indexes in the corresponding axes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JxmxNwfqppil",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df2.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kPdlJSXuJOXh"
   },
   "source": [
    "The method `describe` computes a set of summary statistics for Servies of each DataFrame column: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qq3UUnFrppil"
   },
   "outputs": [],
   "source": [
    "df2.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PSZ35vV5I7hw"
   },
   "source": [
    "Of course, we can transpose this as well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rbf_QhQzppil"
   },
   "outputs": [],
   "source": [
    "df2.describe().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nP3nFLndppil"
   },
   "source": [
    "One simple way of counting/finding non nulls is to just apply the `count()` method on our dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cJqwkM6fJGt0"
   },
   "outputs": [],
   "source": [
    "df2.count() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OnjJMinbppil"
   },
   "source": [
    "### Index objects\n",
    "\n",
    "Pandas’s Index objects are responsible for holding the axis labels and other metadata (like the axis name or names). Any array or other sequence of labels used when constructing a Series or DataFrame is internally converted to an Index, which are immutable and can't be modified by you:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4oq5VB60ppil"
   },
   "outputs": [],
   "source": [
    "df2.index[1] = 'x'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XO1i13lMppil"
   },
   "outputs": [],
   "source": [
    "df2.index[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kGCbxsFGlh_u"
   },
   "source": [
    "Inmutability is important so that Index objects ca be safely shared amongst data structures:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6l1_xctdlKxV"
   },
   "outputs": [],
   "source": [
    "series_temp = pd.Series([1.5, -2.5, 0, 1, 2],df2.index)\n",
    "series_temp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v3ahsl1Bl1Ym"
   },
   "source": [
    "We can also manipulate rows by referencing the index location in a similar manner to what we saw before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F2fhY-Dwppil"
   },
   "outputs": [],
   "source": [
    "df2.iloc[2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QcmvWPKQNbDn"
   },
   "outputs": [],
   "source": [
    "#@title Saving df2 to Drive\n",
    "import os\n",
    "save_to_drive = True #@param {type:\"boolean\"}\n",
    "\n",
    "if save_to_drive and files_loc:\n",
    "  df2.to_pickle(os.path.join(files_loc,\"df2.pkl\"))\n",
    "else:\n",
    "  print('Please, mount Google Drive running the cell at the beginning and try again')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hsLlTuExmEj1"
   },
   "outputs": [],
   "source": [
    "!ls {files_loc}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rhmBydFNMQwN"
   },
   "outputs": [],
   "source": [
    "files_loc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HnCKP-JbJe9X"
   },
   "source": [
    "### More on Loc and iLoc [optional practice]\n",
    "See section 1 of the scrapbook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mt-KihMbppil"
   },
   "source": [
    "### Dropping entries from an axis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v_SOz_6DmD8e"
   },
   "source": [
    "Dropping one or more entries from an axis is easy if you have an index array or list without those entries. As that can require a bit of munging and set logic, the drop method will return a new object with the indicated value or values deleted from an axis.\n",
    "\n",
    "Let's create a new series to demonstrate all this:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cS74cVESppil"
   },
   "outputs": [],
   "source": [
    "s5 = pd.Series(np.arange(5), list('jduvk'))\n",
    "s5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jcjyXadTppil"
   },
   "outputs": [],
   "source": [
    "s6 = s5.drop(['d','k'])\n",
    "s6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uEKNC8aoLULb"
   },
   "source": [
    "Yes, we dropped elements present in the index defined by a list, but can we do just the opposite and keep the elements provided in a list while dropping everything else?:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7MvsJc-IKp1G"
   },
   "outputs": [],
   "source": [
    "s6b = s5[s5.index.intersection(['d','k'])]\n",
    "s6b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RX36lMeyppil"
   },
   "source": [
    "By default, `drop()` doesn't modify the original Series, it creates a copy. We can change that with the argument `inplace` that we'll see later on:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2Vvrs1kfppil"
   },
   "outputs": [],
   "source": [
    "s5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "84z6dJfoppil"
   },
   "outputs": [],
   "source": [
    "s6['u'] = 7\n",
    "s5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LaZMnkManHV1"
   },
   "source": [
    "Let's now work with Dataframes. First, let's see dropping elements from axes in a dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1Sn4-OZnPbLe"
   },
   "outputs": [],
   "source": [
    "#@title Loading df2 from Drive\n",
    "load_from_drive = True #@param {type:\"boolean\"}\n",
    "\n",
    "if load_from_drive and files_loc:\n",
    "  df2 = pd.read_pickle(os.path.join(files_loc,\"df2.pkl\"))\n",
    "else:\n",
    "  print('Please, mount Google Drive running the cell at the beginning and try again')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sbhCe_5yppil"
   },
   "outputs": [],
   "source": [
    "df2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ifct-utn5khx"
   },
   "source": [
    "Using `drop` on the dataframe will operate on the defaul axis 0, meaning that the specified object needs to be a **row**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MnAyJZFgppil"
   },
   "outputs": [],
   "source": [
    "df2.drop('c')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0mu9o9H95Zrg"
   },
   "source": [
    "Let's now select the columns axis (axis 1) to remove a specific **column**, in this case `2nd_language`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ExbGDfv2ppil"
   },
   "outputs": [],
   "source": [
    "df2.drop('2nd_language', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HDEYq3QYnTws"
   },
   "source": [
    "We can see that we didn't modify the dataframe. In fact, we can make a copy to it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T2KVvp2-ppil"
   },
   "outputs": [],
   "source": [
    "df3 = df2.copy()\n",
    "df3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "28ojzuOoNKxg"
   },
   "source": [
    "Yes! the `copy()` method is in fact a deep copy and what we're avoiding is accidentally modifying the original dataframe. Let's have a look with another example (slight detour).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "haIp4ycfNxBn"
   },
   "outputs": [],
   "source": [
    "df_detour = pd.DataFrame({'x': [1,2]})\n",
    "df_sub = df_detour[0:1]\n",
    "df_sub.x = -1\n",
    "df_detour"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QIPiNiv7OIIv"
   },
   "source": [
    "Aha! We even get a warning... but let's continue and see how this, in contrast, leaves df_detour unchanged:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qWyMwVEmOgav"
   },
   "outputs": [],
   "source": [
    "df_detour = pd.DataFrame({'x': [1,2]})\n",
    "df_sub_copy = df_detour[0:1].copy()\n",
    "df_sub_copy.x = -1\n",
    "df_detour"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jY9MbL2Rnh0Z"
   },
   "source": [
    "OK, going back to what we were doing!\n",
    "\n",
    "As mentioned before, let's use the parameter `inplace` to modify the dataframe right away:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2gm56XIWppil"
   },
   "outputs": [],
   "source": [
    "df3.drop('capital', axis=1, inplace=True)\n",
    "df3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s-v9g0Edppim"
   },
   "source": [
    "### Indexing, selection, and filtering\n",
    "\n",
    "The key here is that we can build boolean Series that we can use to index the original Series or DataFrame. Those booleans can be combined with bitwise boolean operators (&, |, ~) to get filters that are as complex as we need. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g5AzFx7X6aAM"
   },
   "source": [
    "Let's revisit our `s3` series that we defined above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yi3r7w0gUez6"
   },
   "outputs": [],
   "source": [
    "#@title Loading s3 from Drive\n",
    "load_from_drive = True #@param {type:\"boolean\"}\n",
    "\n",
    "if load_from_drive and files_loc:\n",
    "  s3 = pd.read_pickle(os.path.join(files_loc,\"s3.pkl\"))\n",
    "else:\n",
    "  print('Please, mount Google Drive running the cell at the beginning and try again')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YscXWNdTppim"
   },
   "outputs": [],
   "source": [
    "s3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ex4mGlbGPbFj"
   },
   "source": [
    "We can select elements from the Series just passing a list of them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zrwOCFSwppim"
   },
   "outputs": [],
   "source": [
    "s3[['Zaragoza', 'Madrid']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wVwvolKdPqYj"
   },
   "source": [
    "Of course, we can use slice notation (remember we start at 0):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ht_JtOOoppim"
   },
   "outputs": [],
   "source": [
    "s3[2:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "43zGkn-8olji"
   },
   "source": [
    "Slicing with labels behaves differently than normal Python slicing in that the endpoint is inclusive:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fgyGiJjjppim"
   },
   "outputs": [],
   "source": [
    "s3['Sevilla':'Cordoba']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eJPofG4tQBMN"
   },
   "source": [
    "But be careful, because by using the index position we default back to good ol' Python behavior (not inclusive):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sWBaYBzZP6pq"
   },
   "outputs": [],
   "source": [
    "s3[1:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GTLoQgFnQSku"
   },
   "source": [
    "We can apply a filter to the whole Series, generating in fact a boolean mask:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oFOMBDGdppim"
   },
   "outputs": [],
   "source": [
    "s3 > 1e06"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2gyfAZd1QZ3r"
   },
   "source": [
    "...and we can pass that boolean mask to the Series itself:\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ivfFJykVppim"
   },
   "outputs": [],
   "source": [
    "s3[s3>1e06]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iPVNdWJdQpZn"
   },
   "source": [
    "Let's work now with our dataframe `df3`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VPLzPopTppim"
   },
   "outputs": [],
   "source": [
    "df3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BpSab-WAQzUT"
   },
   "source": [
    "First, let's build a boolean mask (filter) over the column `year` of our dataframe, which will give us a Series:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j5GLX_PkVe0b"
   },
   "outputs": [],
   "source": [
    "df3['year'] > 1950"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iG_QMk-3Q_u8"
   },
   "source": [
    "And then, let's apply that filter over the full dataframe. The dataframe will match the Series name against its columns and will proceed to filter out the rows (axis 0) that match the criteria:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v2ueGzNQppim"
   },
   "outputs": [],
   "source": [
    "df3[df3['year'] > 1950]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bIQcA_OmRRxn"
   },
   "source": [
    "We can combine all this, making more powerful filters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5V_sbXP5ppim"
   },
   "outputs": [],
   "source": [
    "df3[(df3['year'] > 1900) & (df3['debt'] > 0.5)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OaSANOjdRh8C"
   },
   "source": [
    "We can as well write this in a more elegant and Pythonic way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uP9-n63bppim"
   },
   "outputs": [],
   "source": [
    "recent = df3['year'] > 1900\n",
    "indebted = df3['debt'] > 0.5\n",
    "\n",
    "df3[recent & indebted]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "93lafBDfppim"
   },
   "source": [
    "### Function application and mapping\n",
    "\n",
    "Function application and mapping allows us to modify the elements of a DataFrame (columns with apply or elements with applymap) without for loops. This way we are not constrained to the functions already implemented by pandas or numpy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qp9Cz0VXppim"
   },
   "outputs": [],
   "source": [
    "df3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kwV64i_Lppim"
   },
   "outputs": [],
   "source": [
    "np.sqrt(df3['population'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5THoqTr8ppim"
   },
   "outputs": [],
   "source": [
    "df4 = pd.DataFrame(np.random.randn(4,3) * 17 + 15, columns=list('bde'), index=list('BMPZ'))\n",
    "df4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HueGUX--ppim"
   },
   "outputs": [],
   "source": [
    "np.abs(df4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b0nCT53xppim"
   },
   "source": [
    "This is a typical use case for lambdas (anonymous functions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xr7G6yWCppim"
   },
   "outputs": [],
   "source": [
    "df4.apply(lambda series: series.max() - series.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OOlEdhoDppim"
   },
   "outputs": [],
   "source": [
    "df4.applymap(lambda element: element % 10 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hSOnJwJ_ppim"
   },
   "outputs": [],
   "source": [
    "df4.apply(lambda series: series.max() - series.min(), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YZYr0GHGppim"
   },
   "outputs": [],
   "source": [
    "def f(series):\n",
    "    return pd.Series([series.max(), series.min()], index=['max', 'min'])\n",
    "\n",
    "df4.apply(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hmMMH7-cYmXO"
   },
   "outputs": [],
   "source": [
    "def f2(series):\n",
    "  return pd.Series([series.max() - series.min()], index=['distance'])\n",
    "\n",
    "df4.apply(f2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ykgbRTWpppin"
   },
   "outputs": [],
   "source": [
    "for item in df4.items():\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mazNPgTWppin"
   },
   "outputs": [],
   "source": [
    "for item in df4.iteritems():\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xJcSEmUSppin"
   },
   "outputs": [],
   "source": [
    "map(f, [1,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kFNI1Txtppin"
   },
   "outputs": [],
   "source": [
    "def format_2digits(number):\n",
    "    return '%.2f' % number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P9FxST_Gppin"
   },
   "outputs": [],
   "source": [
    "df4.applymap(format_2digits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NqZ0Rwrbppin"
   },
   "source": [
    "### Sorting and ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KHhk2gpSucyO"
   },
   "outputs": [],
   "source": [
    "df4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "El2uMsJ2ufoL"
   },
   "outputs": [],
   "source": [
    "df4.sort_index(ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Kzl7SFavppin"
   },
   "outputs": [],
   "source": [
    "df4.sort_index(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ltrwa0yUppin"
   },
   "outputs": [],
   "source": [
    "df4.sort_index(ascending=False, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O0cOCqFxppin"
   },
   "outputs": [],
   "source": [
    "df4.sort_values(by='e')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IGJk2X8ovJkk"
   },
   "source": [
    "We can sort by two columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JhWgwkb6ppin"
   },
   "outputs": [],
   "source": [
    "df4.sort_values(by=['e','b'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IYpnn6Itppin"
   },
   "outputs": [],
   "source": [
    "s1 = pd.Series([2,3,8,4,3,2,1], index=list('abcdefg'))\n",
    "s1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SfUXEWBTppin"
   },
   "outputs": [],
   "source": [
    "s1.sort_values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2wxspe6pppin"
   },
   "source": [
    "rank() returns the positions of the elements of the Series in its sorted version. If there are ties, it will take averages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B5Vp8SWtppin"
   },
   "outputs": [],
   "source": [
    "s1.rank()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fQO0yUvtppin"
   },
   "outputs": [],
   "source": [
    "pd.Series([1,1,1]).rank()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5P8zEzh-ppin"
   },
   "outputs": [],
   "source": [
    "s2 = pd.Series([30,10,20], index=list('abc'))\n",
    "s2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LhcM9xS0ppin"
   },
   "outputs": [],
   "source": [
    "s2.rank()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zoByRSJYppin"
   },
   "outputs": [],
   "source": [
    "help(s2.rank)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c0-ekWGippin"
   },
   "source": [
    "#### Exercise\n",
    "\n",
    "Write a function that takes a Series and returns the top 10% registers. In this case, earners. Test it with this Series:\n",
    "\n",
    "```python\n",
    "salaries = pd.Series([150000, 90000, 120000,30000,10000,5000,40000, 50000, 80000, 35000, 27000,14000, 28000, 22000,25000])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bvAMYqtYppin"
   },
   "outputs": [],
   "source": [
    "salaries = pd.Series([150000, 90000, 120000,30000,10000,5000,40000, 50000, 80000, 35000, 27000,14000, 28000, 22000,25000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jo2K9MiSppin"
   },
   "outputs": [],
   "source": [
    "def top_earners(serie):\n",
    "    number_to_extract = round(len(serie) / 10)\n",
    "    return salaries.sort_values()[-number_to_extract:]\n",
    "\n",
    "top_earners(salaries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Dn4I3qsZppin"
   },
   "outputs": [],
   "source": [
    "def top_earners(serie, percentile=0.9):\n",
    "    is_top_earner = serie.rank(pct=True) > percentile\n",
    "    return serie[is_top_earner]\n",
    "\n",
    "print(top_earners(salaries))\n",
    "print(top_earners(salaries, .8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nJA0qwh-ppio"
   },
   "source": [
    "## Summarizing and computing descriptive statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sK2l7AVgppio"
   },
   "outputs": [],
   "source": [
    "x = pd.Series([1.2, np.nan, 4, np.nan, 9], index=list('abcde'))\n",
    "y = pd.Series([5, 3, 7, np.nan, 14], index=list('abcde'))\n",
    "\n",
    "df = pd.DataFrame([x, y], index=['x','y'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mcu_JFjP7bF1"
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame([x, y], index=['x','y']).T\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KuhSUaAsppio"
   },
   "outputs": [],
   "source": [
    "df.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cQgM7hvxppio"
   },
   "source": [
    "As with many methods, we can use them in the direction perpendicular to their default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Qbx5Y9ncppio"
   },
   "outputs": [],
   "source": [
    "df.sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JebyGpE1ppio"
   },
   "outputs": [],
   "source": [
    "pd.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JupE4p1bppio"
   },
   "outputs": [],
   "source": [
    "df.sum(axis=1, skipna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fXwo6Gjappio"
   },
   "outputs": [],
   "source": [
    "df.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fN3J2WFoppio"
   },
   "outputs": [],
   "source": [
    "df.mean(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5GccjzZFppio"
   },
   "outputs": [],
   "source": [
    "df.cumsum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xvZEvY9Appio"
   },
   "outputs": [],
   "source": [
    "df.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Zrb2iyhMppio"
   },
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Qq2kOaZ1ppio"
   },
   "outputs": [],
   "source": [
    "df['x'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H2EY0lK9ppio"
   },
   "outputs": [],
   "source": [
    "df['x'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r0BQNgzwppio"
   },
   "source": [
    "### Unique values, value counts, and membership"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Gp21vVAeppio"
   },
   "outputs": [],
   "source": [
    "s7 = pd.Series(list('gtcaaagcttcga'))\n",
    "s7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oAZRQQI7ppio"
   },
   "outputs": [],
   "source": [
    "s7.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FT_1Azigppio"
   },
   "outputs": [],
   "source": [
    "s7.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DdcGkaruppio"
   },
   "outputs": [],
   "source": [
    "puric_bases = ['a','g']\n",
    "s7.isin(puric_bases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gLgkjn7kppio"
   },
   "outputs": [],
   "source": [
    "s7[s7.isin(puric_bases)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xl3cXv40ppio"
   },
   "source": [
    "## Handling missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6FAt3aaCppip"
   },
   "outputs": [],
   "source": [
    "string_data = pd.Series(['Ma', 'Lu', 'Ca', 'Va', np.nan])\n",
    "string_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jEizNN-zppip"
   },
   "outputs": [],
   "source": [
    "string_data[string_data!=np.nan]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vmh5RQo2ppip"
   },
   "source": [
    "This is weird... but it has some really good reasons. You can find explanations [here](https://stackoverflow.com/questions/10034149/why-is-nan-not-equal-to-nan) and [here](https://stackoverflow.com/questions/1565164/what-is-the-rationale-for-all-comparisons-returning-false-for-ieee754-nan-values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r6ggQeRRppip"
   },
   "outputs": [],
   "source": [
    "np.nan == np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hcNzPfPPppip"
   },
   "outputs": [],
   "source": [
    "string_data[~string_data.isnull()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3SCyqfQ6ppip"
   },
   "source": [
    "### Filtering out missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dcyDAUkTppip"
   },
   "outputs": [],
   "source": [
    "string_data[string_data.notnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pX-pp0zpppip"
   },
   "outputs": [],
   "source": [
    "df5 = pd.DataFrame([[1,2,3], \n",
    "                    [np.nan, 8, 7], \n",
    "                    [4, np.nan, 90], \n",
    "                    [67,42,53]], \n",
    "                   columns=list('abc'))\n",
    "df5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YMXKAHE8ppip"
   },
   "outputs": [],
   "source": [
    "df5[df5['a'].notnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uLTHc7R6ppip"
   },
   "outputs": [],
   "source": [
    "df5.notnull()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SUqU7l95ppip"
   },
   "source": [
    "any() and all() are functions of boolean Series. They reduce the Series to a single boolean value by applying repeatedly the operators \"or\" and \"and\", respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o01r25gTppip"
   },
   "outputs": [],
   "source": [
    "df5.notnull().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "abSg2dafppip"
   },
   "outputs": [],
   "source": [
    "df5.notnull().all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f2BXRR7Yppip"
   },
   "outputs": [],
   "source": [
    "df5.isnull().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IkAzAAzKppip"
   },
   "outputs": [],
   "source": [
    "df5.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0duyP7joppip"
   },
   "outputs": [],
   "source": [
    "df5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lOw3bzYkppip"
   },
   "outputs": [],
   "source": [
    "df5.dropna(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RWRDDzmWppip"
   },
   "outputs": [],
   "source": [
    "array = np.random.randn(8,3) * 20 + 100\n",
    "\n",
    "df6 = pd.DataFrame(array, columns=list('xyz'), index=list('abcdefgh'))\n",
    "df6.iloc[2:5, 1] = np.nan\n",
    "df6.iloc[1:3, 2] = np.nan\n",
    "df6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9VZP8-ohppip"
   },
   "source": [
    "The thresh argument specifies the minimum number of non-null values required to keep a column (or row, with axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Hytnrsdbppip"
   },
   "outputs": [],
   "source": [
    "df6.dropna(thresh=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p6n21pOMppiq"
   },
   "outputs": [],
   "source": [
    "df6.dropna(thresh=2, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YDf39MASppiq"
   },
   "outputs": [],
   "source": [
    "df6.dropna(thresh=6, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0y_jOmuGppiq"
   },
   "source": [
    "### Filling in missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qCFPw5FBppiq"
   },
   "outputs": [],
   "source": [
    "df6.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_2JW5-C9ppiq"
   },
   "outputs": [],
   "source": [
    "df6.fillna({'x' : 100, 'y' : 50, 'z' : 20})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C0OnyZ4Uppiq"
   },
   "outputs": [],
   "source": [
    "df6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OaNb0D5Lppiq"
   },
   "outputs": [],
   "source": [
    "df6.fillna(method='ffill')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0SCc8c6Sppiq"
   },
   "outputs": [],
   "source": [
    "df6.fillna(df6.median())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l_g-RSnUppiq",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df6.median()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "id": "J7qWQyORppiq"
   },
   "source": [
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LgYETBqeppiq"
   },
   "source": [
    "# Loading and saving data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "id": "59gcEppoppiq"
   },
   "source": [
    "## Loading CSV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6nko4hqaIPus"
   },
   "source": [
    "Let's load information coming from the [US government bureau of Transportation Statistics](https://www.transtats.bts.gov/Tables.asp?DB_ID=111). For convenience, I've made this table available for you from Drive:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YqpWOb1E4nut"
   },
   "outputs": [],
   "source": [
    "!wget https://bit.ly/ks-pds-csv3 -O {files_loc}/T100I_SEGMENT_ALL_CARRIER.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GtebGY3Qppiq"
   },
   "source": [
    "Make sure the file is there, and store the path in a Python variable using a Linux shell filter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cVUz4nubJAZW"
   },
   "outputs": [],
   "source": [
    "!ls {files_loc}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3lpRIy6t4AFN"
   },
   "outputs": [],
   "source": [
    "contents = !ls {files_loc}/*csv2*\n",
    "csv_file = contents[0] #storing the first occurrence of the filter, should be our file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qp2kM3ijppiq"
   },
   "outputs": [],
   "source": [
    "trafficDf = pd.read_csv(csv_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NBFCpgJYppiq"
   },
   "outputs": [],
   "source": [
    "len(trafficDf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tipm1vNSppiq"
   },
   "outputs": [],
   "source": [
    "trafficDf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "id": "URoeGvitppiq"
   },
   "source": [
    "## Saving to Excel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4phFUAgSJZ34"
   },
   "source": [
    "Let's save the first 1000 rows of the dataframe in an Excel file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8CTrnFOjppiq"
   },
   "outputs": [],
   "source": [
    "trafficDf.head(1000).to_excel(os.path.join(files_loc, \"excel_output.xls\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FjGbGU4EJg2S"
   },
   "source": [
    "Again, check that the file got generated. If you've got Office, you can test this is indeed a proper excel file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Bbe6LYzD4_bc"
   },
   "outputs": [],
   "source": [
    "!ls {files_loc}/*.xls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "id": "OEeLCrKPppiq"
   },
   "source": [
    "## Saving to CSV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-3JdVdyWJpwL"
   },
   "source": [
    "Now, let's truncate our existing dataframe and save the first 10 rows into another CSV:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cSRCQwO6ppiq",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trafficDf.head(10).to_csv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vRDpkTgappiq"
   },
   "outputs": [],
   "source": [
    "trafficDf.head(1000).to_csv(os.path.join(files_loc, \"out.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rhjegEmp5Q2w"
   },
   "outputs": [],
   "source": [
    "!ls {files_loc}/*.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "id": "XW7oSQpTppir"
   },
   "source": [
    "## To Sql Database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4kJKaduXJzWh"
   },
   "source": [
    "Saving to a database with Pandas is trivial as well. For testing purposes, we'll be using a file-based sqlite3 database that we're creating from code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v4eKRMmhppir"
   },
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "conn = sqlite3.connect(os.path.join(files_loc,'example.db'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fSxVplJOppir"
   },
   "outputs": [],
   "source": [
    "trafficDf.to_sql('traffic',conn, if_exists='replace')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ruBc-XJS50vN"
   },
   "outputs": [],
   "source": [
    "!ls {files_loc}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "id": "xXwtLD3gppir"
   },
   "source": [
    "## To dictionary and to json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ar7hJt1d6SVH"
   },
   "source": [
    "See documentation of [pandas.DataFrame.to_dict](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.to_dict.html) to understand different options for converting DataFrames to dictionaries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VdZvPKKQppir",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trafficDf.head(2).to_dict(orient='records')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8vmoVG1wKAZn"
   },
   "source": [
    "Converting to JSON is quite similar:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f4bfoOOLppir"
   },
   "outputs": [],
   "source": [
    "trafficDf.head(2).to_json(orient='records')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hEhJCaDmppir"
   },
   "source": [
    "## Reading Excel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xDk7w0aRKFbI"
   },
   "source": [
    "To practice reading from Excel, let's load what we saved previously:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NmsLYMrVppir"
   },
   "outputs": [],
   "source": [
    "df2 = pd.read_excel(os.path.join(files_loc, \"excel_output.xls\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FWBY9OMzppir"
   },
   "outputs": [],
   "source": [
    "df2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "63Q9T-GhKMkt"
   },
   "source": [
    "If you explore the columns in the database above, you'll realize we brought back some unexpected visitors (columns named \"Unnamed\", in this case mostly from the own row system in Excel). If you do not want them (for example, to preserve the original dataframe structure that we saved above), you can apply a filter like the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dorWYgBX7LeC"
   },
   "outputs": [],
   "source": [
    "df2 = df2.loc[:, ~df2.columns.str.contains('^Unnamed')]\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hEuOtq217-oH"
   },
   "source": [
    "But we could avoid cleaning up unnamed columns by loading the Excel the right way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w0SwYkZH7QIA"
   },
   "outputs": [],
   "source": [
    "df2 = pd.read_excel(os.path.join(files_loc, \"excel_output.xls\"), index_col=0)\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dun-5ujcKvIJ"
   },
   "source": [
    "### The challenge\n",
    "\n",
    "Let's download data from car accidents in Madrid straight from the source and explore the data a bit to understand the structure. You'll be doing an data consolidation exercise afterwards:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a-nk1-pWppir"
   },
   "outputs": [],
   "source": [
    "!wget https://datos.madrid.es/egob/catalogo/207831-0-accidentes-trafico.xls -P {files_loc}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zEszX_pGLL7t"
   },
   "source": [
    "Again, we store the full file path into a Python variable for convenience:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CX41AYrgppir"
   },
   "outputs": [],
   "source": [
    "contents = !ls {files_loc}/*accidentes*\n",
    "file_path = contents[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J1xcNhhiLSlH"
   },
   "source": [
    "Then, we read the file and explore the structure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HpsEXVgP81IX"
   },
   "outputs": [],
   "source": [
    "df_accidentes = pd.read_excel(file_path)\n",
    "\n",
    "df_accidentes.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IRmf7s2HLXqF"
   },
   "source": [
    "There are plenty of options we can use when reading an Excel file, here are some of them where we're selecting specific ranges of data and a named sheet from the full worksheet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1dlNjCTeppir"
   },
   "outputs": [],
   "source": [
    "df_accidentes = pd.read_excel(file_path, sheet_name='2016', index_col=0, skiprows=7, skipfooter=1, usecols='A:L')\n",
    "\n",
    "df_accidentes.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OtQSUA9DLlTi"
   },
   "source": [
    "Let's have a look at the data from 2010:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SdgVJgH0ppir"
   },
   "outputs": [],
   "source": [
    "accidentes_2010 = pd.read_excel(file_path, \n",
    "                                index_col=0, \n",
    "                                header=7, \n",
    "                                sheet_name='2010', \n",
    "                                skipfooter=1,\n",
    "                                usecols='A:K')\n",
    "accidentes_2010"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C5PL5ielLqiP"
   },
   "source": [
    "Using `sheet_name=None` we load the full workbook. We can then access individual sheets by the sheet name as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qZaaXV8Yppir"
   },
   "outputs": [],
   "source": [
    "all_accidents = pd.read_excel(file_path, index_col=0, header=7, sheet_name=None)\n",
    "all_accidents['2009']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z-44ThzsL9AU"
   },
   "source": [
    "Let's iterate over all the elements of the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M0PyMe2Gppir"
   },
   "outputs": [],
   "source": [
    "for k, v in all_accidents.items():\n",
    "    print(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hJDBC7UxMIvz"
   },
   "source": [
    "What kind of structure are we dealing with here?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "03a3Jhs9EMeI"
   },
   "outputs": [],
   "source": [
    "type(all_accidents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QDoLgB-qMNoH"
   },
   "source": [
    "We printed the values in our loop above. What about the keys? What kind of data structure is the values? Play around a bit with this, you'll need to understand the structure of what you got to tackle the next exercise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NjRiXC49ppir"
   },
   "source": [
    "#### Exercise\n",
    "\n",
    "Consolidate the excel into one DataFrame:\n",
    "- You will need to create a 'YEAR' column.\n",
    "- Think how you can iterate through all the DataFrames.\n",
    "- Explore also where you can take the value of 'YEAR' from, from the comments in the code above it should be clear by now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "id": "4WSKjhx7ppir"
   },
   "source": [
    "## Reading mysql database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O8_zw1wKM47H"
   },
   "source": [
    "Finally, let's read from the SQL database we created before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l9Nl86YOppir"
   },
   "outputs": [],
   "source": [
    "df3 = pd.read_sql_query(\"SELECT * from traffic\", conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ldqbwSYlppir"
   },
   "outputs": [],
   "source": [
    "df3.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gswFnK-Eppir"
   },
   "source": [
    "# Additional References\n",
    "\n",
    "[Python for Data Analysis](http://shop.oreilly.com/product/0636920023784.do)\n",
    "\n",
    "[What is SciPy?](https://www.scipy.org/)\n",
    "\n",
    "[How can SciPy be fast if it is written in an interpreted language like Python?](https://www.scipy.org/scipylib/faq.html#how-can-scipy-be-fast-if-it-is-written-in-an-interpreted-language-like-python)\n",
    "\n",
    "[What is the difference between NumPy and SciPy?](https://www.scipy.org/scipylib/faq.html#what-is-the-difference-between-numpy-and-scipy)\n",
    "\n",
    "[Linear Algebra for AI](https://github.com/fastai/fastai/blob/master/tutorials/linalg_pytorch.ipynb)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Introduction to Pandas - Main Notebook.ipynb",
   "provenance": [
    {
     "file_id": "1U0MM_D7URjf9IQwFd_KHvVIztjw887DS",
     "timestamp": 1611067406767
    },
    {
     "file_id": "1AR961KSJpcMbH6-eDE42FJyRfTmKW7NP",
     "timestamp": 1606582938185
    }
   ],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  },
  "livereveal": {
   "scroll": true,
   "start_slideshow_at": "selected"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
